{
  "hash": "cd54fb01e0077c4630139f6a1f973c04",
  "result": {
    "markdown": "---\neditor: \n  markdown: \n    wrap: 72\n---\n\n```{=tex}\n\\begin{shadequote}[r]{존 체임버스 (John Chambers)}\nR을 이해하려면 두 가지 표어가 도움이 된다: (1) 존재하는 모든 것은 객체다(Everything that exists is an object); (2) 발생한 모든 것은 함수 호출이다.(Everything that happens is a function call.)\n\\end{shadequote}\n```\n\n\n# 대용량 데이터\n\n유닉스 쉘을 통해 데이터를 분석하게 되면 가장 큰 장점이 R/파이썬\n프로그래밍 언어 혹은 엑셀, SAS/SPSS 등과 같은 GUI 프로그램을 열지 않고\n바로 데이터 분석 작업을 수행할 수 있는 것을 꼽을 수 있다. 특히, 데이터\n크기가 수백 MB넘거나 수십GB가 넘는 데이터도 신속하고 빠르게 주어진\n컴퓨팅 자원을 활용하여 결과를 낼 수 있다는 것은 크나큰 장점이다. \n\n## 유일한 대안: CLI\n\n무선통신사는 개인들의 이동전화를 통하여 통신데이터를 주고 받는데, 통신데이터에 기반한 유동인구데이터를 생산한다. 예를 들어, KT는 공공기관과 제휴하여 유동인구데이터를 파일 형태로 제공한다. 월기준으로 경기도 유동인구를 공간정보와 함께 제공하는데 대략 파일크기가 350 MB 가 넘는 크기를 보여주고 있다. 특히, `sex_age_22.csv` 파일은 2022년 경기도 전체 유동인구 데이터로 크가 5.4 GB를 보여주고 있다.\n\\index{KT 유동인구 데이터} \\index{대용량 데이터}\n\n![KT 유동인구 파일형태 데이터](images/big_file.png){fig-align=\"center\" width=\"500\"}\n\n이런 대용량 데이터를 엑셀과 같은 스프레드시트 프로그램에서는 여는 것조차\n불가능하다. `Notepad++`와 같은 텍스트 파일 편집기를 인텔 i5 프로세서가\n장착되고 32GB 주기억장치(메모리, 램)를 갖는 윈도우 10 환경에서 열어보는 것조차\n불가능하다. \\index{Notepad++}\n\n데이터 파일을 열수도 없고 본격적인 분석은 시작도 할 수 없는 상황이다.\n이런 경우 지금까지 학습한 유닉스 쉘을 활용하여 데이터 분석 목적을 달성할\n수 있다. 본격적인 데이터 분석에 들어가기에 앞서 `head`, `tail` 명령어를\n사용하여 데이터 파일 기본상태를 점검한다.\\index{head} \\index{tail}\n\n![`head`와 `tail` 명령어 파일 앞뒤 살펴보기](images/big_file_view.png){fig-align=\"center\" width=\"453\"}\n\n## 공공 데이터셋\n\n챗GPT 프롬프트를 통해 실습에 필요한 적절한 데이터셋을 검색하여 바로 찾을\n수 있다. 이런 점이 기존 구글검색과 큰 차이점이자 향후 구글 검색 대신\n챗GPT를 활용하여 업무에 활용하는 사례가 늘어나게 되는 이유가 아닐까\n싶다. \\index{공공데이터}\n\n::: callout-note\n### 10 MB 이상 오픈 데이터셋\n\n> 파일크기가 10 메가바이트 이상인 오픈 csv 데이터셋을 찾으세요\n\n\\rule{7cm}{0.3mm}\n\n데이터셋을 찾는 과정에서 여러 출처를 확인했으나, 파일 크기가 10\n메가바이트 이상인 CSV 데이터셋에 대한 구체적인 정보를 제공하는\n웹사이트를 찾지 못했습니다. 하지만, 여러 공개 데이터 웹사이트가\n존재하며, 이들 웹사이트에서 큰 CSV 파일을 찾을 수 있을 것입니다. 예를\n들면, Data.gov, World Bank Open Data, Uniform Crime Reporting Program\n등이 있습니다.\n\n따라서, 이러한 웹사이트에서 관심 있는 주제의 데이터셋을 검색하고, 필요에\n따라 다운로드를 진행해 보시는 것이 좋겠습니다. 아쉽게도 현재로서는\n파일의 크기를 미리 확인하는 방법이 제한적입니다. 한 가지 방법은\n데이터셋을 다운로드 받아 직접 파일의 크기를 확인하는 것입니다.\n\n제가 찾아본 결과 중에는 GitHub에 저장된 9.54 메가바이트 크기의 CSV\n파일이 있었습니다. 이 파일의 URL은 다음과 같습니다:\n\n... 중략\n\n이 파일이 사용자의 요구를 충족시킬 수 있을지는 별도로 확인이 필요합니다.\n\n더 구체적인 데이터셋을 찾기 위해서는 사용자의 관심사나 필요한 데이터의\n종류에 대한 추가 정보가 필요합니다. 이를 통해 저는 더 구체적인 검색을\n진행하고 필요한 데이터셋을 찾는 데 도움이 될 수 있습니다.\n:::\n\n2020년부터 미국 캘리포니아주의 로스앤젤레스(LA)시에서 발생한 범죄 사건이 담긴 범죄데이터를 DATA.GOV \\footnote{https://catalog.data.gov/dataset/crime-data-from-2020-to-present}에서 `.csv` 파일 형태로 다운로드 받는다. LA 범죄데이터에 대한 데이터 사전도\n웹페이지에 잘 정리되어 있다. \\footnote{https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8}\n\n`Crime_Data_from_2020_to_Present.csv` 파일은 237 MB로 수 GB크기를 갖지는\n않지만 실습을 위한 대용량 데이터로 사용하기에 적합해보인다.\n\n``` bash\n$ du -sh data/*\n237M    data/Crime_Data_from_2020_to_Present.csv\n8.0K    data/penguin_adelie.csv\n4.0K    data/penguin_chinstrap.csv\n8.0K    data/penguin_gentoo.csv\n20K     data/penguins.csv\n```\n\nLA 범죄 데이터에 첫번째 궁금한 점은 LA에서 일어나는 범죄중에 우리나라와\n비교하여 빈도수가 높은 범죄를 파악하는 것이다. 이를 위해서 데이터\n사전에서 \"Crm Cd\", \"Crm Cd Desc\" 두 칼럼이 범죄유형코드와 범죄명이\n기술되어 있음을 확인할 수 있다. \\index{LA 범죄 데이터셋}\n\n프롬프트를 직관적으로 작성하여 챗GPT 결과를 보고 이를 실제 데이터에\n반영하여 결과를 확인해보자.\n\n::: callout-note\n### LA 범죄데이터 범죄유형 프롬프트\n\n> https://catalog.data.gov/dataset/crime-data-from-2020-to-present\n> 웹사이트에서 2020년부터 현재까지 LA 범죄데이터셋이 있습니다.\n> 범죄유형별(\"Crm Cd Des\") 빈도수를 내림차순 정렬하는 쉘 코드를\n> 작성해주세요.\n\n\\rule{7cm}{0.3mm}\n\n먼저, 해당 데이터셋을 다운로드하고 CSV 형식이라고 가정하겠습니다. 그리고\n\"Crm Cd Des\"라는 이름의 열이 범죄 유형을 나타내는 것으로 이해하겠습니다.\n\n파일이 crime_data.csv라는 이름으로 다운로드되었다고 가정하면, 우리는\nawk, sort, 그리고 uniq 명령어를 사용하여 범죄 유형별 빈도수를 계산하고\n내림차순으로 정렬할 수 있습니다.\n\n다음은 이를 수행하는 쉘 스크립트입니다:\n\n``` bash\nawk -F ',' '{print $X}' crime_data.csv | sort | uniq -c | sort -nr\n```\n\n여기서 \\$X는 \"Crm Cd Des\" 열의 위치입니다. 이 위치는 CSV 파일의 구조에\n따라 다를 수 있으므로, 실제 열 번호로 대체해야 합니다. 예를 들어, \"Crm\nCd Des\"가 CSV 파일의 5번째 열이라면, 스크립트는 다음과 같이 될 것입니다:\n\n``` bash\nawk -F ',' '{print $5}' crime_data.csv | sort | uniq -c | sort -nr\n```\n\n이 스크립트는 다음을 수행합니다:\n\n1.  awk -F ',' '{print \\$5}' crime_data.csv: CSV 파일을 읽고 각 행에서\n    5번째 열(즉, \"Crm Cd Des\")을 출력합니다.\n2.  sort: awk에서 출력한 범죄 유형을 알파벳순으로 정렬합니다.\n3.  uniq -c: 정렬된 범죄 유형의 리스트에서 고유한 항목을 찾아 각 항목의\n    빈도수를 계산합니다.\n4.  sort -nr: 빈도수를 기준으로 결과를 내림차순으로 정렬합니다.\n\n이 코드는 간단한 CSV 파일에 대해서는 잘 작동하지만, 데이터에 쉼표나\n따옴표 등이 포함된 경우에는 제대로 작동하지 않을 수 있습니다. 이런\n경우에는 보다 복잡한 파싱을 수행하는 도구(예: Python의 pandas\n라이브러리)를 사용해야 할 수도 있습니다.\n:::\n\n출력결과가 너무 길어 `head` 명령어로 범죄빈도가 높은 10개만 뽑아보자.\n\n``` bash\n$ awk -F ',' '{print $10}' data/Crime_Data_from_2020_to_Present.csv | sort | uniq -c | sort -nr | head\n  95762 VEHICLE - STOLEN\n  78340 BATTERY - SIMPLE ASSAULT\n  63197 BURGLARY FROM VEHICLE\n  58921 \"VANDALISM - FELONY ($400 & OVER\n  58497 THEFT OF IDENTITY\n  58189 BURGLARY\n  53785 \"ASSAULT WITH DEADLY WEAPON\n  53735 THEFT PLAIN - PETTY ($950 & UNDER)\n  49939 INTIMATE PARTNER - SIMPLE ASSAULT\n  39442 THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)\n```\n\n동일한 작업을 R을 활용하여 검증해 보면 동일한 결과가 얻어진 것이\n확인된다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrimes <- read_csv(\"data/Crime_Data_from_2020_to_Present.csv\")\n\ncrimes %>%\n  janitor::clean_names() %>%\n  count(crm_cd, crm_cd_desc, sort = TRUE)\n  \n#> # A tibble: 141 × 3\n#>    crm_cd crm_cd_desc                                                 n\n#>     <dbl> <chr>                                                   <int>\n#>  1    510 VEHICLE - STOLEN                                        95762\n#>  2    624 BATTERY - SIMPLE ASSAULT                                78340\n#>  3    330 BURGLARY FROM VEHICLE                                   63197\n#>  4    740 VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS) 58921\n#>  5    354 THEFT OF IDENTITY                                       58497\n#>  6    310 BURGLARY                                                58189\n#>  7    230 ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT          53785\n#>  8    440 THEFT PLAIN - PETTY ($950 & UNDER)                      53735\n#>  9    626 INTIMATE PARTNER - SIMPLE ASSAULT                       49939\n#> 10    420 THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)         39442\n#> # ℹ 131 more rows\n#> # ℹ Use `print(n = ...)` to see more rows  \n```\n:::\n\n\n\n고급 데이터 분석을 위한 유닉스 쉘 스크립트를 작성하는 것은 R 혹은 파이썬\n코드를 작성하는 것과 비교하여 난이도가 있지만 챗GPT의 도움을 받아 공동\n개발하게 되면 비교적 수월하게 원하는 결과를 얻을 수 있다. 유닉스 쉘\n데이터 분석 장점은 아무래도 특별한 도구(R/파이썬, 엑셀, SAS/SPSS 등)를\n사용하지 않고도 컴퓨터에 필수적으로 설치되어 있는 운영체제에서 간단한\n분석부터 고급 분석작업까지 가볍고 빠르게 수행할 수 있는 장점이 있다.\n데이터 과학을 시작하는 분들에게 오랜 숙련의 시간이 필요하고 거대한\n장벽처럼 느껴졌던 CLI 대용량 데이터 분석이 챗GPT의 도입으로 우리곁에\n성금 다가섰고, 데이터 과학분야에서 가장 큰 수혜주는 아마도 유닉스 쉘로\n판단된다.\n\n## 데이터베이스\n\n데이터베이스(Database)는 데이터를 저장, 검색, 조작, 갱신하기 위한\n시스템으로, 정형화된 데이터를 테이블 형태로 저장하며, 각 테이블은\n열(속성)과 행(레코드)로 구성되고 외래키를 이용하여 관계를 구성한다.\n데이터 과학에서 데이터베이스를 사용하는 경우로 다음을 꼽을 수 있다:\n\n-   대용량 데이터 관리: 데이터베이스는 대용량의 데이터를 효율적으로\n    관리하는 가장 이상적인 방식으로, 안정적이고 효율적인 방식으로\n    데이터를 저장하며, 필요한 경우 쉽게 SQL 언어로 검색하고 추출할 수\n    있다.\n\n-   데이터 무결성 보장: 데이터베이스는 데이터 정확성과 일관성을\n    보장함으로서 데이터 무결성을 보장한다.\n\n-   보안 유지: 데이터베이스는 사용자에게 적절한 권한을 부여하여 데이터에\n    대한 접근을 통제함으로써 데이터 보안을 강화시키고 데이터\n    프라이버시(Data Privacy)도 강화한다.\n\n-   데이터 분석 기능: 데이터베이스는 SQL 쿼리 언어를 통해 다양한\n    질의문(Query) 작성을 지원한다. 이른 통해 데이터 과학자는 필요한\n    데이터를 빠르게 찾고 분석하는데 시간을 단축시킬 수 있다.\n\nDuckDB\\footnote{[DuckDB: Quacking SQL, \"Lost in Translation between R and Python\n8\"](https://lorentzen.ch/index.php/2022/04/02/duckdb-quacking-sql/)}는\n\"SQLite for Analytics\" 별명을 갖고 있다. 과거 OLAP(OnLine Analytical\nProcessing)이 OLTP(OnLine Transaction Processing)와 함께 회자되던 시절이\n있었고, 그 OLAP을 구현하는 것으로 이해할 수 있다. \\index{데이터베이스}\n\\index{DuckDB}\n\n### 데이터셋\n\n[NYC Taxi Trip Data - Google Public\nData](https://www.kaggle.com/datasets/neilclack/nyc-taxi-trip-data-google-public-data)\n데이터셋은 구글 빅쿼리(Bigquery) 공개 데이터셋중 일부로 뉴욕택시 운행기록 천만건을 담고 있다. 뉴욕 택시 데이터셋에 대한 자세한 정보는 캐글 웹사이트 (kaggle.com)에서 확인할 수 있다. \\index{뉴욕 택시 데이터}\n\n### 데이터베이스\n\n`duckdb`패키지를 설치하여 `taxis.duckdb` 파일 데이터베이스로 생성하고\n연결시켜둔다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(DBI)\nlibrary(vroom)\nlibrary(tictoc)\n\ndatabase_path <- paste0(here::here(), \"/data/taxis.duckdb\")\nfile.remove(database_path)\ncon <- dbConnect(duckdb(), dbdir = database_path)\ndbListTables(con) \n```\n:::\n\n\n\n`data\\` 디렉토리 아래 뉴욕 택시 운행 데이터와 `duckdb`가 하나 파일명으로\n`taxis.duckdb` 생성된 것이 확인된다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfs::dir_tree(\"data\")\n#> data\n#> +-- Crime_Data_from_2020_to_Present.csv\n#> +-- epubple\n#> |   +-- PDF파일 원고 예시.pdf\n#> |   +-- 워드파일 원고 예시.docx\n#> |   \\-- 한글파일 원고 예시.hwp\n#> +-- fonts\n#> |   +-- Font Awesome 6 Brands-Regular-400.otf\n#> |   +-- Font Awesome 6 Free-Regular-400.otf\n#> |   \\-- Font Awesome 6 Free-Solid-900.otf\n#> +-- original_cleaned_nyc_taxi_data_2018.csv\n#> +-- penguins.csv\n#> +-- penguin_adelie.csv\n#> +-- penguin_chinstrap.csv\n#> +-- penguin_gentoo.csv\n#> \\-- taxis.duckdb\n```\n:::\n\n\n\n### 테이블 추가\n\n`duckdb` 데이터베이스에 뉴욕택시 데이터셋을 테이블로 추가한다.\n데이터프레임 형태를 갖는 `.csv` 파일은 그 자체로 데이터베이스 테이블로\n1:1 변환이 가능하다. `read_csv_auto()` 함수를 사용해서 `.csv` 파일을\n데이터베이스로 불러읽은 후 테이블 생성시킨다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxis_path <- paste0(here::here(), \"/data/original_cleaned_nyc_taxi_data_2018.csv\")\ntable_create_qry <- glue::glue(\n  \"CREATE TABLE trips AS SELECT * FROM read_csv_auto ('{taxis_path}')\"\n  )\ndbExecute(con, table_create_qry)\n```\n:::\n\n\n\n```         \n[1] 8319928\n```\n\n### 테이블 확인\n\n`dbListTables()` 명령어로 데이터베이스 내 테이블이 제대로 올라갔는지\n확인한다. 앞서 `trips`로 테이블명을 정의했기 때문에 `taxis.duckdb`\n데이터베이스 내 테이블로 동일한 명칭으로 확인된다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbListTables(con)\n```\n:::\n\n\n\n```         \n[1] \"trips\"\n```\n\n### DB 연결 끊기\n\n데이터베이스는 기본적으로 다수 사용자가 사용하는 자원으로 데이터베이스\n연결한 후 자원을 반환하지 않는 경우 접속자수 제한으로 접속이 되지 않는\n피해자가 발생할 수 있고, 보안문제도 야기한다. 따라서, `con`으로 DB에\n연결을 했고, 목적을 달성한 후에 `dbDisconnect()` 명령어로 연결을\n해제시켜 자원을 반환하여 다른 사용자가 원활히 사용할 수 있도록 배려한다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con, shutdown=TRUE)\n```\n:::\n\n\n\n### SQL 쿼리\n\n지금까지 작업한 내용은 데이터베이스에 접근한 후 빈 데이터베이스에 `.csv`\n파일을 올려 테이블을 생성한 후 테이블이 정상적으로 존재함을 확인한 후에\n데이터베이스 연결을 해제했다. 데이터베이스에 접근하여 데이터 분석 작업을\n수행하는 일반적인 작업흐름은 다음과 같다.\n\n1.  파일 `duckDB` 데이터베이스를 생성한다.\n2.  데이터베이스에 `con` DB 핸들러를 통해 R/파이썬 연결한다.\n3.  CSV 파일을 테이블로 데이터베이스에 등록한다.\n4.  정상적으로 테이블이 데이터베이스에 등록되었는지를 확인한다.\n5.  쿼리문을 작성하여 데이터를 추출한 후 데이터 과학 분석작업을\n    수행한다.\n6.  DB 핸들러를 반납하고 데이터베이스 연결을 해제한다.\n\n![](images/duckdb_nyc_taxi.jpg){fig-align=\"center\" width=\"533\"}\n\n이제부터 본격적으로 OLAP 분석작업을 수행한다. 파일 \"/data/taxis.duckdb\"\n데이터베이스에 DB 핸들러를 연결시킨다. 그리고 나서 분석대상 테이블이\n존재하는지 `dbListTables()` 명령어로 확인한다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(DBI)\nlibrary(vroom)\nlibrary(tictoc)\n\ndatabase_path <- paste0(here::here(), \"/data/taxis.duckdb\")\n\ncon <-dbConnect(duckdb::duckdb(), dbdir = database_path, read_only=TRUE)\n\ndbListTables(con)\n#> [1] \"trips\"\n```\n:::\n\n\n\n`tictok` 패키지를 통해 해당 쿼리가 수행되는데 실행된 시간을 측정한다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\n\nfare_summary <- con |> \n  tbl(\"trips\") |> \n  dplyr::select(payment_type, fare_amount, trip_distance) |> \n  filter(trip_distance > 18) |> \n  group_by(payment_type) |> \n  summarise(average_fare = mean(fare_amount, na.rm = TRUE)) |> \n  collect()\n\ntoc()\n#> 0.33 sec elapsed\n\nfare_summary\n#> # A tibble: 4 x 2\n#>   payment_type average_fare\n#>          <dbl>        <dbl>\n#> 1            1         65.4\n#> 2            2         60.9\n#> 3            3         62.5\n#> 4            4         67.1\n```\n:::\n\n\n\n### `dplyr` → SQL 쿼리문\n\n`dplyr` 데이터 조작 문법이 매우 직관적이고 사용하기 편하지만, 대용량\n데이터를 조작하는 경우 속도가 중요하기 때문에 SQL 쿼리문으로 실행하는\n경우가 많다. 이와 같이 코딩의 편리함과 빠른 속도를 동시에 추구하는 경우\n`dplyr` 데이터 문법으로 작성된 코드를 SQL문으로 변환하게 되면 한 단계\n과정을 더 거쳐 문제를 해결할 수 있다. 향후, 안정화된 경우 이를\n스크립트로 작성하여 자동화하면 중간에 번거러운 과정도 깔끔하게 정리할 수\n있다. `dplyr` 데이터 문법을 SQL 쿼리문으로 작성하는 방법은\n`show_query()` 명령어로 내보내면 된다. 이제 동일한 SQL 문으로 작성되었기\n때문에 쿼리문을 실행시켜 분석작업을 수행하자. \\index{dplyr}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbplyr)\n\ntrips_db <- tbl(con, \"trips\")\n\ntrips_sql_query <- trips_db %>% \n  dplyr::select(payment_type, fare_amount, trip_distance) %>% \n  filter(trip_distance > 18) %>% \n  group_by(payment_type) %>% \n  summarise(average_fare = mean(fare_amount, na.rm = TRUE)) %>% \n  show_query()\n#> <SQL>\n#> SELECT payment_type, AVG(fare_amount) AS average_fare\n#> FROM (\n#>   SELECT payment_type, fare_amount, trip_distance\n#>   FROM trips\n#>   WHERE (trip_distance > 18.0)\n#> ) q01\n#> GROUP BY payment_type\n\ntrips_sql_query\n#> # Source:   SQL [4 x 2]\n#> # Database: DuckDB 0.8.1 [statkclee@Windows 10 x64:R 4.2.2/D:/tcs/gpt-shell/data/taxis.duckdb]\n#>   payment_type average_fare\n#>          <dbl>        <dbl>\n#> 1            1         65.4\n#> 2            2         60.9\n#> 3            3         62.5\n#> 4            4         67.1\n```\n:::\n\n\n\n동일한 결과를 `dbGetQuery()`함수로 결과값을 얻을 수 있다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql_query_from_dbplyr <- \"SELECT payment_type, AVG(fare_amount) AS average_fare\nFROM (\n  SELECT payment_type, fare_amount, trip_distance\n  FROM trips\n) q01\nWHERE (trip_distance > 18.0)\nGROUP BY payment_type\"\n\n# dbGetQuery(con, \"SELECT * FROM trips LIMIT 5;\")\ndbGetQuery(con, sql_query_from_dbplyr)\n#>   payment_type average_fare\n#> 1            1     65.40479\n#> 2            2     60.86640\n#> 3            3     62.52643\n#> 4            4     67.13895\n```\n:::\n\n\n\n### DBeaver SQL 쿼리 도구\n\n[DBeaver Community - Free Universal Database\nTool](https://dbeaver.io/download/) 도구를 다운로드 받고 앞서 구축한\n뉴욕 택시 데이터베이스를 연결한 후 SQL 쿼리문을 실행시키게 되면 동일한\n결과를 얻을 수 있다. \\index{쿼리 도구} \\index{DBeaver} 먼저, [DBeaver Community - Free Universal Database\nTool](https://dbeaver.io/download/) 웹사이트에서 운영체제에 맞는 SQL\n쿼리 도구를 설치한다. 그리고 나서 앞서 [dbplyr](https://dbplyr.tidyverse.org/) `show_query()`\n함수를 사용해서 SQL 문을 복사하여 붙여넣기 하면 해당 결과를 얻을 수\n있다.\n\n![DBeaver SQL 쿼리 도구 화면](images/dplyr_to_dbeaver.png){fig-align=\"center\" width=\"500\"}\n\n### 파일 크기와 속도\n\n뉴욕 택시 원본파일 크기를 살펴보자. 이를 위해서 `fs`패키지\n`file_info()`함수를 사용해서 확인한다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfs::file_info(\"data/original_cleaned_nyc_taxi_data_2018.csv\") %>% \n  select(path, type, size)\n#> # A tibble: 1 x 3\n#>   path                                         type         size\n#>   <fs::path>                                   <fct> <fs::bytes>\n#> 1 data/original_cleaned_nyc_taxi_data_2018.csv file         719M\n```\n:::\n\n\n\nCSV 파일을 duckDB에서 가져왔을 때 데이터베이스 크기를 살펴보자.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfs::file_info(\"data/taxis.duckdb\") %>% \n  select(path, type, size)\n#> # A tibble: 1 x 3\n#>   path              type         size\n#>   <fs::path>        <fct> <fs::bytes>\n#> 1 data/taxis.duckdb file         248M\n```\n:::\n\n\n\n원본 `.csv` 파일 크기가 700 MB가 넘는데 `.duckdb`로 변환시키면 파일크가\n대략 250 MB로 대폭 줄어든 것이 확인된다. 주목할 점은 여러 데이터 조작\n기법이 적용된 제법 난이도 있는 쿼리문을 실행시간이 `0.11`초에 불과하다는\n점이다. 데이터 조작 쿼리를 정확하고 빠르게 작성하는 것 못지않게 빠른\n컴퓨팅 속도는 데이터 분석가나 데이터 엔지니어에게 꼭 필요하다. 왜냐하면\n그 어떤 것보다 시간은 소중하기 때문이다.\n\n## 데이터베이스와 쿼리도구\n\n데이터베이스 활용사례가 늘어나면서 그 목적에 특화된 데이터베이스로\n분화되었다. 각 데이터베이스와 궁합이 맞는 다양한 SQL 쿼리도구가 존재하여\n많이 사용되는 데이터베이스와 SQL 쿼리도구를 @tbl-rdbms-query 에\n정리하였다.\n\n\n| 순             | 데이터베이스     | SQL 쿼리도구     |\n|:--------------:|:----------------:|:----------------:|\n| 1              | SQLite DuckDB    | DBeaver          |\n| 2              | MySQL            | HeidiSQL / MySQL Workbench |\n| 3              | postgreSQL       | pgAdmin          |\n\n: 관계형 데이터베이스와 SQL 쿼리도구 {#tbl-rdbms-query}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}