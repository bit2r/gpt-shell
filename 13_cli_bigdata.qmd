---
editor: 
  markdown: 
    wrap: 72
---

```{=tex}
\begin{shadequote}[r]{존 체임버스 (John Chambers)}
R을 이해하려면 두 가지 표어가 도움이 된다: (1) 존재하는 모든 것은 객체다(Everything that exists is an object); (2) 발생한 모든 것은 함수 호출이다.(Everything that happens is a function call.)
\end{shadequote}
```
# 대용량 데이터

유닉스 쉘을 통해 데이터를 분석하게 되면 가장 큰 장점이 R/파이썬
프로그래밍 언어 혹은 엑셀, SAS/SPSS 등과 같은 GUI 프로그램을 열지 않고
바로 데이터 분석 작업을 수행할 수 있는 것을 꼽을 수 있다. 특히, 데이터
크기가 수백 MB넘거나 수십GB가 넘는 데이터도 신속하고 빠르게 주어진
컴퓨팅 자원을 활용하여 결과를 낼 수 있다는 것은 크나큰 장점이다. [@marwick2018packaging] [@boettiger2015introduction]

## 유일한 대안: CLI

무선통신사는 개인들의 이동전화를 통하여 통신데이터를 주고 받는데, 통신데이터에 기반한 유동인구데이터를 생산한다. 예를 들어, KT는 공공기관과 제휴하여 유동인구데이터를 파일 형태로 제공한다. 월기준으로 경기도 유동인구를 공간정보와 함께 제공하는데 대략 파일크기가 350 MB 가 넘는 크기를 보여주고 있다. 특히, `sex_age_22.csv` 파일은 2022년 경기도 전체 유동인구 데이터로 크가 5.4 GB를 보여주고 있다.
\index{KT 유동인구 데이터} \index{대용량 데이터}

![KT 유동인구 파일형태 데이터](images/big_file.png){fig-align="center" width="500"}

이런 대용량 데이터를 엑셀과 같은 스프레드시트 프로그램에서는 여는 것조차
불가능하다. `Notepad++`와 같은 텍스트 파일 편집기를 인텔 i5 프로세서가
장착되고 32GB 주기억장치(메모리, 램)를 갖는 윈도우 10 환경에서 열어보는 것조차
불가능하다. \index{Notepad++}

데이터 파일을 열수도 없고 본격적인 분석은 시작도 할 수 없는 상황이다.
이런 경우 지금까지 학습한 유닉스 쉘을 활용하여 데이터 분석 목적을 달성할
수 있다. 본격적인 데이터 분석에 들어가기에 앞서 `head`, `tail` 명령어를
사용하여 데이터 파일 기본상태를 점검한다.\index{head} \index{tail}

![`head`와 `tail` 명령어 파일 앞뒤 살펴보기](images/big_file_view.png){fig-align="center" width="453"}

## 공공 데이터셋

챗GPT 프롬프트를 통해 실습에 필요한 적절한 데이터셋을 검색하여 바로 찾을
수 있다. 이런 점이 기존 구글검색과 큰 차이점이자 향후 구글 검색 대신
챗GPT를 활용하여 업무에 활용하는 사례가 늘어나게 되는 이유가 아닐까
싶다. \index{공공데이터}

::: callout-note
### 10 MB 이상 오픈 데이터셋

> 파일크기가 10 메가바이트 이상인 오픈 csv 데이터셋을 찾으세요

\rule{7cm}{0.3mm}

데이터셋을 찾는 과정에서 여러 출처를 확인했으나, 파일 크기가 10
메가바이트 이상인 CSV 데이터셋에 대한 구체적인 정보를 제공하는
웹사이트를 찾지 못했습니다. 하지만, 여러 공개 데이터 웹사이트가
존재하며, 이들 웹사이트에서 큰 CSV 파일을 찾을 수 있을 것입니다. 예를
들면, Data.gov, World Bank Open Data, Uniform Crime Reporting Program
등이 있습니다.

따라서, 이러한 웹사이트에서 관심 있는 주제의 데이터셋을 검색하고, 필요에
따라 다운로드를 진행해 보시는 것이 좋겠습니다. 아쉽게도 현재로서는
파일의 크기를 미리 확인하는 방법이 제한적입니다. 한 가지 방법은
데이터셋을 다운로드 받아 직접 파일의 크기를 확인하는 것입니다.

제가 찾아본 결과 중에는 GitHub에 저장된 9.54 메가바이트 크기의 CSV
파일이 있었습니다. 이 파일의 URL은 다음과 같습니다:

... 중략

이 파일이 사용자의 요구를 충족시킬 수 있을지는 별도로 확인이 필요합니다.

더 구체적인 데이터셋을 찾기 위해서는 사용자의 관심사나 필요한 데이터의
종류에 대한 추가 정보가 필요합니다. 이를 통해 저는 더 구체적인 검색을
진행하고 필요한 데이터셋을 찾는 데 도움이 될 수 있습니다.
:::

2020년부터 미국 캘리포니아주의 로스앤젤레스(LA)시에서 발생한 범죄 사건이 담긴 범죄데이터를 DATA.GOV \footnote{https://catalog.data.gov/dataset/crime-data-from-2020-to-present}에서 `.csv` 파일 형태로 다운로드 받는다. LA 범죄데이터에 대한 데이터 사전도
웹페이지에 잘 정리되어 있다. \footnote{https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8}

`Crime_Data_from_2020_to_Present.csv` 파일은 237 MB로 수 GB크기를 갖지는
않지만 실습을 위한 대용량 데이터로 사용하기에 적합해보인다.

``` bash
$ du -sh data/*
237M    data/Crime_Data_from_2020_to_Present.csv
8.0K    data/penguin_adelie.csv
4.0K    data/penguin_chinstrap.csv
8.0K    data/penguin_gentoo.csv
20K     data/penguins.csv
```

LA 범죄 데이터에 첫번째 궁금한 점은 LA에서 일어나는 범죄중에 우리나라와
비교하여 빈도수가 높은 범죄를 파악하는 것이다. 이를 위해서 데이터
사전에서 "Crm Cd", "Crm Cd Desc" 두 칼럼이 범죄유형코드와 범죄명이
기술되어 있음을 확인할 수 있다. \index{LA 범죄 데이터셋}

프롬프트를 직관적으로 작성하여 챗GPT 결과를 보고 이를 실제 데이터에
반영하여 결과를 확인해보자.

::: callout-note
### LA 범죄데이터 범죄유형 프롬프트

> https://catalog.data.gov/dataset/crime-data-from-2020-to-present
> 웹사이트에서 2020년부터 현재까지 LA 범죄데이터셋이 있습니다.
> 범죄유형별("Crm Cd Des") 빈도수를 내림차순 정렬하는 쉘 코드를
> 작성해주세요.

\rule{7cm}{0.3mm}

먼저, 해당 데이터셋을 다운로드하고 CSV 형식이라고 가정하겠습니다. 그리고
"Crm Cd Des"라는 이름의 열이 범죄 유형을 나타내는 것으로 이해하겠습니다.

파일이 crime_data.csv라는 이름으로 다운로드되었다고 가정하면, 우리는
awk, sort, 그리고 uniq 명령어를 사용하여 범죄 유형별 빈도수를 계산하고
내림차순으로 정렬할 수 있습니다.

다음은 이를 수행하는 쉘 스크립트입니다:

``` bash
awk -F ',' '{print $X}' crime_data.csv | sort | uniq -c | sort -nr
```

여기서 \$X는 "Crm Cd Des" 열의 위치입니다. 이 위치는 CSV 파일의 구조에
따라 다를 수 있으므로, 실제 열 번호로 대체해야 합니다. 예를 들어, "Crm
Cd Des"가 CSV 파일의 5번째 열이라면, 스크립트는 다음과 같이 될 것입니다:

``` bash
awk -F ',' '{print $5}' crime_data.csv | sort | uniq -c | sort -nr
```

이 스크립트는 다음을 수행합니다:

1.  awk -F ',' '{print \$5}' crime_data.csv: CSV 파일을 읽고 각 행에서
    5번째 열(즉, "Crm Cd Des")을 출력합니다.
2.  sort: awk에서 출력한 범죄 유형을 알파벳순으로 정렬합니다.
3.  uniq -c: 정렬된 범죄 유형의 리스트에서 고유한 항목을 찾아 각 항목의
    빈도수를 계산합니다.
4.  sort -nr: 빈도수를 기준으로 결과를 내림차순으로 정렬합니다.

이 코드는 간단한 CSV 파일에 대해서는 잘 작동하지만, 데이터에 쉼표나
따옴표 등이 포함된 경우에는 제대로 작동하지 않을 수 있습니다. 이런
경우에는 보다 복잡한 파싱을 수행하는 도구(예: Python의 pandas
라이브러리)를 사용해야 할 수도 있습니다.
:::

출력결과가 너무 길어 `head` 명령어로 범죄빈도가 높은 10개만 뽑아보자.

``` bash
$ awk -F ',' '{print $10}' data/Crime_Data_from_2020_to_Present.csv | sort | uniq -c | sort -nr | head
  95762 VEHICLE - STOLEN
  78340 BATTERY - SIMPLE ASSAULT
  63197 BURGLARY FROM VEHICLE
  58921 "VANDALISM - FELONY ($400 & OVER
  58497 THEFT OF IDENTITY
  58189 BURGLARY
  53785 "ASSAULT WITH DEADLY WEAPON
  53735 THEFT PLAIN - PETTY ($950 & UNDER)
  49939 INTIMATE PARTNER - SIMPLE ASSAULT
  39442 THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)
```

동일한 작업을 R을 활용하여 검증해 보면 동일한 결과가 얻어진 것이
확인된다.

```{r}
#| eval: false

crimes <- read_csv("data/Crime_Data_from_2020_to_Present.csv")

crimes %>%
  janitor::clean_names() %>%
  count(crm_cd, crm_cd_desc, sort = TRUE)
  
#> # A tibble: 141 × 3
#>    crm_cd crm_cd_desc                                                 n
#>     <dbl> <chr>                                                   <int>
#>  1    510 VEHICLE - STOLEN                                        95762
#>  2    624 BATTERY - SIMPLE ASSAULT                                78340
#>  3    330 BURGLARY FROM VEHICLE                                   63197
#>  4    740 VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS) 58921
#>  5    354 THEFT OF IDENTITY                                       58497
#>  6    310 BURGLARY                                                58189
#>  7    230 ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT          53785
#>  8    440 THEFT PLAIN - PETTY ($950 & UNDER)                      53735
#>  9    626 INTIMATE PARTNER - SIMPLE ASSAULT                       49939
#> 10    420 THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)         39442
#> # ℹ 131 more rows
#> # ℹ Use `print(n = ...)` to see more rows  
```

고급 데이터 분석을 위한 유닉스 쉘 스크립트를 작성하는 것은 R 혹은 파이썬
코드를 작성하는 것과 비교하여 난이도가 있지만 챗GPT의 도움을 받아 공동
개발하게 되면 비교적 수월하게 원하는 결과를 얻을 수 있다. 유닉스 쉘
데이터 분석 장점은 아무래도 특별한 도구(R/파이썬, 엑셀, SAS/SPSS 등)를
사용하지 않고도 컴퓨터에 필수적으로 설치되어 있는 운영체제에서 간단한
분석부터 고급 분석작업까지 가볍고 빠르게 수행할 수 있는 장점이 있다.
데이터 과학을 시작하는 분들에게 오랜 숙련의 시간이 필요하고 거대한
장벽처럼 느껴졌던 CLI 대용량 데이터 분석이 챗GPT의 도입으로 우리곁에
성금 다가섰고, 데이터 과학분야에서 가장 큰 수혜주는 아마도 유닉스 쉘로
판단된다.

## 데이터베이스

데이터베이스(Database)는 데이터를 저장, 검색, 조작, 갱신하기 위한
시스템으로, 정형화된 데이터를 테이블 형태로 저장하며, 각 테이블은
열(속성)과 행(레코드)로 구성되고 외래키를 이용하여 관계를 구성한다.
데이터 과학에서 데이터베이스를 사용하는 경우로 다음을 꼽을 수 있다:

-   대용량 데이터 관리: 데이터베이스는 대용량의 데이터를 효율적으로
    관리하는 가장 이상적인 방식으로, 안정적이고 효율적인 방식으로
    데이터를 저장하며, 필요한 경우 쉽게 SQL 언어로 검색하고 추출할 수
    있다.

-   데이터 무결성 보장: 데이터베이스는 데이터 정확성과 일관성을
    보장함으로서 데이터 무결성을 보장한다.

-   보안 유지: 데이터베이스는 사용자에게 적절한 권한을 부여하여 데이터에
    대한 접근을 통제함으로써 데이터 보안을 강화시키고 데이터
    프라이버시(Data Privacy)도 강화한다.

-   데이터 분석 기능: 데이터베이스는 SQL 쿼리 언어를 통해 다양한
    질의문(Query) 작성을 지원한다. 이른 통해 데이터 과학자는 필요한
    데이터를 빠르게 찾고 분석하는데 시간을 단축시킬 수 있다.

DuckDB\footnote{[DuckDB: Quacking SQL, "Lost in Translation between R and Python
8"](https://lorentzen.ch/index.php/2022/04/02/duckdb-quacking-sql/)}는
"SQLite for Analytics" 별명을 갖고 있다. 과거 OLAP(OnLine Analytical
Processing)이 OLTP(OnLine Transaction Processing)와 함께 회자되던 시절이
있었고, 그 OLAP을 구현하는 것으로 이해할 수 있다. \index{데이터베이스}
\index{DuckDB}

### 데이터셋

[NYC Taxi Trip Data - Google Public
Data](https://www.kaggle.com/datasets/neilclack/nyc-taxi-trip-data-google-public-data)
데이터셋은 구글 빅쿼리(Bigquery) 공개 데이터셋중 일부로 뉴욕택시 운행기록 천만건을 담고 있다. 뉴욕 택시 데이터셋에 대한 자세한 정보는 캐글 웹사이트 (kaggle.com)에서 확인할 수 있다. \index{뉴욕 택시 데이터}

### 데이터베이스

`duckdb`패키지를 설치하여 `taxis.duckdb` 파일 데이터베이스로 생성하고
연결시켜둔다.

```{r}
#| eval: false
library(tidyverse)
library(duckdb)
library(DBI)
library(vroom)
library(tictoc)

database_path <- paste0(here::here(), "/data/taxis.duckdb")
file.remove(database_path)
con <- dbConnect(duckdb(), dbdir = database_path)
dbListTables(con) 
```

`data\` 디렉토리 아래 뉴욕 택시 운행 데이터와 `duckdb`가 하나 파일명으로
`taxis.duckdb` 생성된 것이 확인된다.

```{r}
fs::dir_tree("data")
```

### 테이블 추가

`duckdb` 데이터베이스에 뉴욕택시 데이터셋을 테이블로 추가한다.
데이터프레임 형태를 갖는 `.csv` 파일은 그 자체로 데이터베이스 테이블로
1:1 변환이 가능하다. `read_csv_auto()` 함수를 사용해서 `.csv` 파일을
데이터베이스로 불러읽은 후 테이블 생성시킨다.

```{r}
#| eval: false
taxis_path <- paste0(here::here(), "/data/original_cleaned_nyc_taxi_data_2018.csv")
table_create_qry <- glue::glue(
  "CREATE TABLE trips AS SELECT * FROM read_csv_auto ('{taxis_path}')"
  )
dbExecute(con, table_create_qry)
```

```         
[1] 8319928
```

### 테이블 확인

`dbListTables()` 명령어로 데이터베이스 내 테이블이 제대로 올라갔는지
확인한다. 앞서 `trips`로 테이블명을 정의했기 때문에 `taxis.duckdb`
데이터베이스 내 테이블로 동일한 명칭으로 확인된다.

```{r}
#| eval: false
dbListTables(con)
```

```         
[1] "trips"
```

### DB 연결 끊기

데이터베이스는 기본적으로 다수 사용자가 사용하는 자원으로 데이터베이스
연결한 후 자원을 반환하지 않는 경우 접속자수 제한으로 접속이 되지 않는
피해자가 발생할 수 있고, 보안문제도 야기한다. 따라서, `con`으로 DB에
연결을 했고, 목적을 달성한 후에 `dbDisconnect()` 명령어로 연결을
해제시켜 자원을 반환하여 다른 사용자가 원활히 사용할 수 있도록 배려한다.

```{r}
#| eval: false
dbDisconnect(con, shutdown=TRUE)
```

### SQL 쿼리

지금까지 작업한 내용은 데이터베이스에 접근한 후 빈 데이터베이스에 `.csv`
파일을 올려 테이블을 생성한 후 테이블이 정상적으로 존재함을 확인한 후에
데이터베이스 연결을 해제했다. 데이터베이스에 접근하여 데이터 분석 작업을
수행하는 일반적인 작업흐름은 다음과 같다.

1.  파일 `duckDB` 데이터베이스를 생성한다.
2.  데이터베이스에 `con` DB 핸들러를 통해 R/파이썬 연결한다.
3.  CSV 파일을 테이블로 데이터베이스에 등록한다.
4.  정상적으로 테이블이 데이터베이스에 등록되었는지를 확인한다.
5.  쿼리문을 작성하여 데이터를 추출한 후 데이터 과학 분석작업을
    수행한다.
6.  DB 핸들러를 반납하고 데이터베이스 연결을 해제한다.

![](images/duckdb_nyc_taxi.jpg){fig-align="center" width="533"}

이제부터 본격적으로 OLAP 분석작업을 수행한다. 파일 "/data/taxis.duckdb"
데이터베이스에 DB 핸들러를 연결시킨다. 그리고 나서 분석대상 테이블이
존재하는지 `dbListTables()` 명령어로 확인한다.

```{r}
library(tidyverse)
library(duckdb)
library(DBI)
library(vroom)
library(tictoc)

database_path <- paste0(here::here(), "/data/taxis.duckdb")

con <-dbConnect(duckdb::duckdb(), dbdir = database_path, read_only=TRUE)

dbListTables(con)
```

`tictok` 패키지를 통해 해당 쿼리가 수행되는데 실행된 시간을 측정한다.

```{r}
tic()

fare_summary <- con |> 
  tbl("trips") |> 
  dplyr::select(payment_type, fare_amount, trip_distance) |> 
  filter(trip_distance > 18) |> 
  group_by(payment_type) |> 
  summarise(average_fare = mean(fare_amount, na.rm = TRUE)) |> 
  collect()

toc()

fare_summary
```

### `dplyr` → SQL 쿼리문

`dplyr` 데이터 조작 문법이 매우 직관적이고 사용하기 편하지만, 대용량
데이터를 조작하는 경우 속도가 중요하기 때문에 SQL 쿼리문으로 실행하는
경우가 많다. 이와 같이 코딩의 편리함과 빠른 속도를 동시에 추구하는 경우
`dplyr` 데이터 문법으로 작성된 코드를 SQL문으로 변환하게 되면 한 단계
과정을 더 거쳐 문제를 해결할 수 있다. 향후, 안정화된 경우 이를
스크립트로 작성하여 자동화하면 중간에 번거러운 과정도 깔끔하게 정리할 수
있다. `dplyr` 데이터 문법을 SQL 쿼리문으로 작성하는 방법은
`show_query()` 명령어로 내보내면 된다. 이제 동일한 SQL 문으로 작성되었기
때문에 쿼리문을 실행시켜 분석작업을 수행하자. \index{dplyr}

```{r}
library(dbplyr)

trips_db <- tbl(con, "trips")

trips_sql_query <- trips_db %>% 
  dplyr::select(payment_type, fare_amount, trip_distance) %>% 
  filter(trip_distance > 18) %>% 
  group_by(payment_type) %>% 
  summarise(average_fare = mean(fare_amount, na.rm = TRUE)) %>% 
  show_query()

trips_sql_query
```

동일한 결과를 `dbGetQuery()`함수로 결과값을 얻을 수 있다.

```{r}
sql_query_from_dbplyr <- "SELECT payment_type, AVG(fare_amount) AS average_fare
FROM (
  SELECT payment_type, fare_amount, trip_distance
  FROM trips
) q01
WHERE (trip_distance > 18.0)
GROUP BY payment_type"

# dbGetQuery(con, "SELECT * FROM trips LIMIT 5;")
dbGetQuery(con, sql_query_from_dbplyr)
```

### DBeaver SQL 쿼리 도구

[DBeaver Community - Free Universal Database
Tool](https://dbeaver.io/download/) 도구를 다운로드 받고 앞서 구축한
뉴욕 택시 데이터베이스를 연결한 후 SQL 쿼리문을 실행시키게 되면 동일한
결과를 얻을 수 있다. \index{쿼리 도구} \index{DBeaver} 먼저, [DBeaver Community - Free Universal Database
Tool](https://dbeaver.io/download/) 웹사이트에서 운영체제에 맞는 SQL
쿼리 도구를 설치한다. 그리고 나서 앞서 [dbplyr](https://dbplyr.tidyverse.org/) `show_query()`
함수를 사용해서 SQL 문을 복사하여 붙여넣기 하면 해당 결과를 얻을 수
있다.

![DBeaver SQL 쿼리 도구 화면](images/dplyr_to_dbeaver.png){fig-align="center" width="500"}

### 파일 크기와 속도

뉴욕 택시 원본파일 크기를 살펴보자. 이를 위해서 `fs`패키지
`file_info()`함수를 사용해서 확인한다.

```{r}
fs::file_info("data/original_cleaned_nyc_taxi_data_2018.csv") %>% 
  select(path, type, size)
```

CSV 파일을 duckDB에서 가져왔을 때 데이터베이스 크기를 살펴보자.

```{r}
fs::file_info("data/taxis.duckdb") %>% 
  select(path, type, size)
```

원본 `.csv` 파일 크기가 700 MB가 넘는데 `.duckdb`로 변환시키면 파일크가
대략 250 MB로 대폭 줄어든 것이 확인된다. 주목할 점은 여러 데이터 조작
기법이 적용된 제법 난이도 있는 쿼리문을 실행시간이 `0.11`초에 불과하다는
점이다. 데이터 조작 쿼리를 정확하고 빠르게 작성하는 것 못지않게 빠른
컴퓨팅 속도는 데이터 분석가나 데이터 엔지니어에게 꼭 필요하다. 왜냐하면
그 어떤 것보다 시간은 소중하기 때문이다.

## 데이터베이스와 쿼리도구

데이터베이스 활용사례가 늘어나면서 그 목적에 특화된 데이터베이스로
분화되었다. 각 데이터베이스와 궁합이 맞는 다양한 SQL 쿼리도구가 존재하여
많이 사용되는 데이터베이스와 SQL 쿼리도구를 @tbl-rdbms-query 에
정리하였다.


| 순             | 데이터베이스     | SQL 쿼리도구     |
|:--------------:|:----------------:|:----------------:|
| 1              | SQLite DuckDB    | DBeaver          |
| 2              | MySQL            | HeidiSQL / MySQL Workbench |
| 3              | postgreSQL       | pgAdmin          |

: 관계형 데이터베이스와 SQL 쿼리도구 {#tbl-rdbms-query}
