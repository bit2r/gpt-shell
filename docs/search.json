[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 유닉스 쉘",
    "section": "",
    "text": "감사의 글\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n기술적인 부분에서 깊은 통찰력을 제공해주신 한국 R 사용자회 유충현 회장님, 챗GPT가 국내에 소개된 후 가장 먼저 챗GPT를 세밀하게 조명할 수 있도록 교육 기회를 주신 경기도의회 문승호, 이자형, 장한별 의원님, 광명시 박승원 시장님, 김규식 부시장님, 김종업 센터장님, 지방행정연구원 최인수, 전대욱, 김필 박사님, 경기도청 AI빅데이터 산업과 이수재 과장님, 원금동, 최정환, 윤여찬 팀장님, 안양시청 엄순용 주무관님, 순천향대학교 박여진 팀장님, 경기도 인재개발원 박선현 주무관님, NICE 디앤알 박정우 대표님, 남영민 본부장님, 우민호 실장님, 서울교육청 조희연 교육감님, 양신호 원장님, 김선자 장학관님, 서울고 학부모회 오혜석님, 디플래닉스 김범진 전대표님, 장석호 대표님, 이용빈님, 경기도 경제과학진흥원 임문영 상임이사님, 건국대 미래지식교육원 이영범 원장님께 감사드립니다.\n이 책의 공개와 출판이 가능했던 것은 한국 R 사용자회의 지원 덕분입니다. 행정사법인 광화문 최순영 대표님, 한채민 과장님, 법무법인 평안 김형주 변호사님, 법률사무소 하우림 정병운 변호사님, 홍성학 감사님, 김호성님, 김현철님, 형환희님, 한국텍학회 김강수님, 경상국립대 백원희 교수님께 진심으로 감사드립니다.\n한국 R 사용자회 활동에서 소프트웨어 카펜트리의 영향은 결코 무시할 수 없습니다. 소프트웨어 카펜트리를 설립한 Greg Wilson 박사님, 카펜트리 재단의 Kari Jordan 박사님, AsiaR 커뮤니티를 이끌고 계신 Janani Ravi 박사님, 서울 R 미트업에서 발표해주신 제빈 웨스트 교수님, 그리고 곽수영, 장연훈, 김송규, 나성호, 안영찬, 박남호, 공병규, 김용우, 이현진 교수님께 감사의 말씀을 드립니다.\n지적 자극을 주시고 더 넓은 세상을 보게 해주신 국가교육위원회 이배용 위원장님, 정대화 상임위원님, 김수환 전문위원님, 삼정 KPMG 장지수 부대표님, 박문구 전무님, 가톨릭 의대 문건웅 교수님, 성균관대 최재성 교수님, 국무조정실 장명헌 사무관님, 제주대 안도현 교수님, 명지대 박순만 교수님, 이희정 전대표님, 고길곤 교수님, 이원일 교수님에게 깊은 감사의 말씀을 전합니다.\n공공정책분야의 빅데이터 분석 활성화를 격려해주신 노웅래 의원님, 김병욱 의원님, 성남시의회 조정식 의원님, 환경보전협회 남광우 전 상근부회장님, 공공의창 최정묵 박사님, 조원씨앤아이 김대진 대표님, 그리고 이 책 출판에 관심이 많으신 오마이뉴스의 김지현 기자님, 나눔국민운동본부 위정희 이사님에게 감사의 인사를 전합니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.",
    "crumbs": [
      "감사의 글"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html",
    "href": "04-shell-pipefilter.html",
    "title": "4  파이프와 필터",
    "section": "",
    "text": "4.1 명령어에서 출력 캡처하기\n파일 중에서 어느 파일이 가장 짧을까요? 단지 6개의 파일이 있기 때문에 질문에 답하기는 쉬울 것이다. 하지만 만약에 6,000개 파일이 있다면 어떨까요? 해결에 이르는 첫번째 단계로 다음 명령을 실행한다:\n&gt; 기호는 쉘로 하여금 처리 결과를 화면에 뿌리는 대신, 파일로 방향변경(redirect)하게 한다. 만약 파일이 존재하지 않으면 파일을 생성하고 파일이 존재하면 파일에 내용을 덮어쓰기 한다. 조용하게 덮어쓰기를 하기 때문에 자료가 유실될 수 있어 주의가 요구된다. ls lengths.txt 을 통해 파일이 존재하는 것을 확인한다:\ncat lengths.txt을 사용해서 화면으로 lengths.txt의 내용을 보낼 수 있다. cat은 “concatenate”(연결, 함께 연결)의 줄임말로 하나씩 하나씩 파일의 내용을 출력한다. 이번 경우에는 파일이 하나만 있어, cat 명령어는 한 파일이 담고 있는 내용만 보여준다:",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html#pipefilter-capture",
    "href": "04-shell-pipefilter.html#pipefilter-capture",
    "title": "4  파이프와 필터",
    "section": "",
    "text": "$ wc -l *.pdb &gt; lengths.txt\n\n$ ls lengths.txt\n\nlengths.txt\n\n$ cat lengths.txt\n\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total\n\n\n\n\n\n\n페이지 단위 출력결과 살펴보기\n\n\n\n편리성과 일관성을 위해 cat 명령어를 계속 사용하지만, 파일 전체를 화면에 쭉 뿌린다는 면에서 단점도 있다. 실무적으로 less 명령어가 더 유용한데 $ less lengths.txt와 같이 사용한다. 파일을 화면 단위로 출력한다. 아래로 내려가려면 스페이스바를 누르고, 뒤로 돌아가려면 b를 누르면 되고, 빠져 나가려면 q를 누른다.",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html#pipefilter-filtering",
    "href": "04-shell-pipefilter.html#pipefilter-filtering",
    "title": "4  파이프와 필터",
    "section": "4.2 출력 필터링",
    "text": "4.2 출력 필터링\n이제 sort 명령어를 사용해서 파일 내용을 정렬한다. 먼저 정렬 명령에 대해 조금 알아보기 위해 연습문제를 풀어보자. \n\n예제 4.1 (sort -n 명령어는 어떤 작업을 수행할까?) 다음 파일 행을 포함하고 있는 파일에 sort 명령어를 실행하면, \n10\n2\n19\n22\n6\n출력결과는 다음과 같다:\n10\n19\n2\n22\n6\n동일한 입력에 대해서 sort -n을 실행하면, 대신에 다음 결과를 얻게 된다:\n2\n6\n10\n19\n22\n인수 -n이 왜 이런 효과를 가지는지 설명하세요.\n\n\n\n\n\n\n해답과 설명\n\n\n\n-n 플래그는 알파벳 정렬이 아닌, 숫자 기준으로 정렬하도록 명세한다.\n\n\n\n-n 플래그를 사용해서 알파벳 대신에 숫자 방식으로 정렬할 것을 지정할 수 있다. 이 명령어는 파일 자체를 변경하지 않고 대신에 정렬된 결과를 화면으로 보낸다:\n$ sort -n lengths.txt\n\n  9  methane.pdb\n 12  ethane.pdb\n 15  propane.pdb\n 20  cubane.pdb\n 21  pentane.pdb\n 30  octane.pdb\n107  total\n&gt; lengths.txt을 사용해서 wc 실행결과를 lengths.txt에 넣었듯이, 명령문 다음에 &gt; sorted-lengths.txt을 넣음으로서, 임시 파일이름인 sorted-lengths.txt에 정렬된 목록 정보를 담을 수 있다. 이것을 실행한 다음에, 또 다른 head 명령어를 실행해서 sorted-lengths.txt에서 첫 몇 행을 뽑아낼 수 있다: \n$ sort -n lengths.txt &gt; sorted-lengths.txt\n$ head -n 1 sorted-lengths.txt\n\n  9  methane.pdb\nhead에 -n 1 매개변수를 사용해서 파일의 첫번째 행만이 필요하다고 지정한다. -n 20은 처음 20개 행만을 지정한다. sorted-lengths.txt이 가장 작은 것에서부터 큰 것으로 정렬된 파일 길이 정보를 담고 있어서, head의 출력 결과는 가장 짧은 행을 가진 파일이 되어야만 된다.\n\n\n\n\n\n\n동일한 파일로 방향변경\n\n\n\n명령어 출력결과를 방향변경하는데 동일한 파일에 보내는 것은 매우 나쁜 생각이다. 예를 들어:\n$ sort -n lengths.txt &gt; lengths.txt\n위와 같이 작업하게 되면 틀린 결과를 얻을 수 있을 뿐만 아니라 경우에 따라서는 lengths.txt 파일 자체 내용을 잃어버릴 수도 있다.\n\n\n\n예제 4.2 (&gt;&gt;은 무엇을 의미하는가?) &gt; 사용법을 살펴봤지만, 유사한 연산자로 &gt;&gt;도 있는데 다소 다른 방식으로 동작한다. 문자열을 출력하는 echo 명령어를 사용해서, 아래 명령을 테스트하여 두 연산자 차이점을 확인한다: \n$ echo hello &gt; testfile01.txt\n$ echo hello &gt;&gt; testfile02.txt\n힌트: 각 명령문을 연속해서 두번 실행하고 나서, 출력결과로 나온 파일을 면밀히 조사한다.\n\n\n\n\n\n\n해답과 설명\n\n\n\n&gt; 연산자를 갖는 첫번째 예제에서 문자열 “hello”는 testfile01.txt 파일에 저장된다. 하지만, 매번 명령어를 실행할 때마다 동일한 문자열 “hello”가 파일에 덮어쓰기 된다.\n두번째 예제에서 &gt;&gt; 연산자도 마찬가지로 “hello”를 파일에 저장(이 경우 testfile02.txt)하는 것을 알 수 있다. 하지만, 파일이 이미 존재하는 경우(즉, 두번째 명령어를 실행하게 되면) 파일에 문자열을 덧붙인다. 계속 반복할 경우 “hello”가 파일에 줄바꿈하여 계속 추가된다.\n\n\n\n\n예제 4.3 (데이터 덧붙이기) head 명령어는 이미 살펴봤고, 파일 시작하는 몇줄을 화면에 출력하는 역할을 수행한다. tail 명령어도 유사하지만, 반대로 파일 마지막 몇줄을 화면에 출력하는 역할을 수행한다. shell-lesson-data/exercise-data/animal-counts/animals.csv 파일을 생각해 보자. 다음 명령어를 실행하게 되면 animals-subset.csv 파일에 저장될 내용이 어떤 것일지 아래에서 정답을 고르세요: \n$ head -n 3 animals.csv &gt; animals-subset.csv\n$ tail -n 2 animals.csv &gt;&gt; animals-subset.csv\n\nanimals.csv 파일 첫 3줄.\nanimals.csv 파일 마지막 2줄.\nanimals.csv 파일의 첫 3줄과 마지막 2줄.\nanimals.csv 파일의 두번째 세번째 줄.\n\n\n\n\n\n\n\n해답과 설명\n\n\n\n정답은 3. 1번이 정답이 되려면, head 명령어만 실행한다. 2번이 정답이 되려면, tail 명령어만 실행한다. 4번이 정답이 되려면, head -3 animals.csv | tail -2 &gt;&gt; animals-subset.csv 명령어를 실행해서 head 출력결과를 파이프에 넣어 tail -2를 실행해야 한다.",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html#pipefilter-redirection",
    "href": "04-shell-pipefilter.html#pipefilter-redirection",
    "title": "4  파이프와 필터",
    "section": "4.3 다른 명령에 출력 전달하기",
    "text": "4.3 다른 명령에 출력 전달하기\n줄이 가장 적은 파일을 찾는 예제에서 출력을 저장하기 위해 lengths.txt와 sorted-lengths.txt 파일을 중간에 사용했다. 하지만, 이런 방식은 번잡한하다. 이유는 wc, sort, head 명령어 각각이 어떻게 동작하는지 이해해도, 중간에 산출되는 파일에 무슨 일이 진행되고 있는지 따라가기는 쉽지 않기 때문이다. sort와 head을 함께 실행하는 경우 이해하기 훨씬 쉽게 만들 수 있다:\n$ sort -n lengths.txt | head -n 1\n\n  9  methane.pdb\n두 명령문 사이의 수직 막대를 파이프(pipe)라고 부른다. 수직막대는 쉘에게 왼편 명령문의 출력결과를 오른쪽 명령문의 입력값으로 사용된다는 뜻을 전달한다. 컴퓨터는 필요하면 임시 파일을 생성하거나, 한 프로그램에서 주기억장치의 다른 프로그램으로 데이터를 복사하거나, 혹은 완전히 다른 작업을 수행할 수도 있다. 사용자는 알 필요도 없고 관심을 가질 이유도 없다. 중요한 것은 중간에 생성된 sorted-lengths.txt 파일이 필요없게 되었다는 점이다.",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html#동작방식",
    "href": "04-shell-pipefilter.html#동작방식",
    "title": "4  파이프와 필터",
    "section": "4.4 동작방식",
    "text": "4.4 동작방식\n파이프를 생성할 때 뒤에서 실질적으로 일어나는 일은 다음과 같다. 컴퓨터가 한 프로그램(어떤 프로그램도 동일)을 실행할 때 프로그램에 대한 소프트웨어와 현재 상태 정보를 담기 위해서 주기억장치 메모리에 프로세스(process)를 생성한다. 모든 프로세스는 표준 입력(standard input)이라는 입력 채널을 가지고 있다. (여기서 이름이 너무 기억하기 좋아서 놀랄지도 모른다. 하지만 걱정하지 말자. 대부분의 유닉스 프로그래머는 “stdin”이라고 부른다). 또한 모든 프로세스는 표준 출력(standard output)(혹은 “stdout”)이라고 불리는 기본디폴트 출력 채널도 있다. 이 채널이 일반적으로 오류 혹은 진단 메시지 용도로 사용되어서 터미널로 오류 메시지를 받으면서도 그 와중에 프로그램 출력값이 또다른 프로그램에 파이프되어 들어가는 것이 가능하게 한다. \n쉘은 실질적으로 또다른 프로그램이다. 정상적인 상황에서 사용자가 키보드로 타이핑하는 모든 것은 표준 입력으로 쉘에 보내지고, 표준 출력에서 만들어지는 무엇이 든지 화면에 출력된다. 쉘에게 프로그램을 실행하게 할때, 새로운 프로세스를 생성하고, 임시로 키보드에 타이핑하는 무엇이든지 그 프로세스의 표준 입력으로 보내지고, 프로세스는 표준 출력을 무엇이든 화면에 전송한다.\nwc -l *.pdb &gt; lengths을 실행할 때 여기서 일어나는 것을 설명하면 다음과 같다. wc 프로그램을 실행할 새로운 프로세스를 생성하라고 쉘이 컴퓨터에 지시한다. 파일이름을 인자로 제공했기 때문에 표준입력 대신 wc는 인자에서 입력값을 읽어온다. &gt;을 사용해서 출력값을 파일로 방향변경 했기 때문에, 쉘은 프로세스의 표준 출력결과를 파일에 연결한다.\nwc -l *.pdb | sort -n을 실행한다면, 쉘은 프로세스 두개를 생성한다. (파이프 프로세스 각각에 대해서 하나씩) 그래서 wc과 sort은 동시에 실행된다. wc의 표준출력은 직접적으로 sort의 표준 입력으로 들어간다. &gt;같은 방향변경이 없기 때문에 sort의 출력은 화면으로 나가게 된다. wc -l *.pdb | sort -n | head -1을 실행하면, 파일에서 wc에서 sort로, sort에서 head을 통해 화면으로 나가게 되는 데이터 흐름을 가진 프로세스 3개가 있게 된다.",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html#pipefilter-together",
    "href": "04-shell-pipefilter.html#pipefilter-together",
    "title": "4  파이프와 필터",
    "section": "4.5 여러 명령 결합하기",
    "text": "4.5 여러 명령 결합하기\n어떤 것도 파이프를 연속적으로 사슬로 엮어 사용하는 것을 막을 수는 없다. 즉, 예를 들어 또 다른 파이프를 사용해서 wc의 출력결과를 sort에 바로 보내고 나서, 다시 처리 결과를 head에 보낸다. wc 출력결과를 sort로 보내는데 파이프를 사용했다:\n$ wc -l *.pdb | sort -n\n\n   9 methane.pdb\n  12 ethane.pdb\n  15 propane.pdb\n  20 cubane.pdb\n  21 pentane.pdb\n  30 octane.pdb\n 107 total\n또 다른 파이프를 사용해서 wc의 출력결과를 sort에 바로 보내고 나서, 다시 처리 결과를 head로 보내게 되면 전체 파이프라인은 다음과 같이 된다:\n$ wc -l *.pdb | sort -n | head -n 1\n\n   9  methane.pdb\n이것이 정확하게 수학자가 log(3x) 같은 중첩함수를 사용하는 것과 같다. “log(3x)은 x에 3을 곱하고 로그를 취하는 것과 같다.” 이번 경우는, *.pdb의 행수를 세어서 정렬해서 첫부분만 계산하는 것이 된다. 시각적으로 그림 4.1 에 표현되어 있다.\n\n\n\n\n\n\n그림 4.1: 방향변경과 파이프\n\n\n\n\n예제 4.4 (명령문을 파이프로 연결하기) 현재 작업 디렉토리에, 최소 행수를 갖는 파일을 3개 찾고자 한다. 아래 열거된 어떤 명령어 중 어떤 것이 원하는 파일 3개를 찾아줄까?\n\nwc -l * &gt; sort -n &gt; head -n 3\nwc -l * | sort -n | head -n 1-3\nwc -l * | head -n 3 | sort -n\nwc -l * | sort -n | head -n 3\n\n\n\n\n\n\n\n해답과 설명\n\n\n\n해답은 4. 파이프 문자 |을 사용해서 이 프로세스 표준출력을 다른 프로세스 표준입력으로 넣어준다. &gt; 기호는 표준입력을 파일로 방향변경할 때 사용한다. shell-lesson-data/exercise-data/alkanes 디렉토리에서 확인해 보자.",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html#pipefilter-why",
    "href": "04-shell-pipefilter.html#pipefilter-why",
    "title": "4  파이프와 필터",
    "section": "4.6 함께 작동하도록 설계된 도구",
    "text": "4.6 함께 작동하도록 설계된 도구\n프로그램을 연결하는 아이디어가 왜 유닉스가 그토록 성공적이었는지를 잘 보여준다. 다양한 작업을 수행하는 거대한 프로그램을 생성하는 대신에, 유닉스 개발자는 각자 한가지 작업만을 아주 잘 수행하는 간단한 도구를 많이 생성하고, 상호 유기적으로 잘 작동하는데 집중한다. 이러한 프로그래밍 모델을 파이프와 필터(pipes and filters)라고 부른다. 파이프는 이미 살펴봤고, 필터(filter)는 입력 스트림을 출력 스트림으로 변환하는 wc, sort같은 프로그램이다. 거의 모든 표준 유닉스 도구는 이런 방식으로 동작한다: 별도로 언급되지 않는다면, 표준 입력에서 읽고, 읽은 것을 가지고 무언가를 수행하고 표준출력에 쓴다.\n중요한 점은 표준입력에서 텍스트 행을 읽고, 표준 출력에 텍스트 행을 쓰는 거의 모든 프로그램이 이런 방식으로 동작하는 모든 다른 프로그램과 조합될 수 있다는 점이다. 여러분도 본인이 작성한 프로그램을 이러한 방식으로 작성할 수 있어야 하고 작성해야 한다. 그래서 여러분과 다른 사람들이 이러한 프로그램을 파이프에 넣어 생태계 전체 힘을 배가할 수 있다.\n\n예제 4.5 (파이프 독해능력) shell-lesson-data/exercise-data/animal-counts 폴더에 animals.csv로 불리는 파일은 다음 데이터를 포함하고 있다.\n2012-11-05,deer,5\n2012-11-05,rabbit,22\n2012-11-05,raccoon,7\n2012-11-06,rabbit,19\n2012-11-06,deer,2\n2012-11-06,fox,4\n2012-11-07,rabbit,16\n2012-11-07,bear,1\n다음 아래 파이프라인에 각 파이프를 통과하고, 마지막 방향변경을 마친 텍스트는 무엇이 될까요? sort -r 명령어는 역방향으로 정렬함에 주목한다.\n$ cat animals.csv | head -n 5 | tail -n 3 | sort -r &gt; final.txt\n힌트: 명령어를 한번에 하나씩 작성해서 파이프라인을 구축한 뒤에 이해한 것이 맞는지 시험한다.\n\n\n\n\n\n\n해답과 설명\n\n\n\nhead 명령어는 animals.csv 파일에서 첫 5 행을 추출한다. 그리고 나서, tail 명령어로 이전 5 행에서 마지막 3 행을 추출된다. sort -r 명령어는 역순으로 정렬을 시키게 된다. 마지막으로 출력결과는 final.txt 파일에 방향변경하여 화면이 아닌 파일로 보낸다. 파일에 저장된 내용은 cat final.txt 명령어를 실행하면 확인이 가능하다. 파일에는 다음 내용이 저장되어야 한다:\n2012-11-06,rabbit,19\n2012-11-06,deer,2\n2012-11-05,raccoon,7\n\n\n\n\n예제 4.6 (파이프 구성) 이전 예제에 사용된 animals.csv 파일을 가지고 다음 명령어를 실행한다:\n$ cut -d , -f 2 animals.csv\n콤마를 구분자로 각 행을 쪼개려고 하면 -d 플래그를 사용하고, -f 플래그는 각행의 두번째 필드를 지정하게 되서 출력결과는 다음과 같다:\ncut 명령은 파일에서 각 행의 특정 부분을 제거하거나 ’잘라내기’하는 데 사용되며, cut 명령어는 행을 탭(Tab) 문자를 구분자로 사용하여 열을 쪼갠다. 이러한 방식으로 사용되는 문자를 구분 기호(delimiter)라고 합니다. 앞에서 -d 옵션을 사용하여 쉼표(,)를 구분 기호 문자로 지정했다. 또한 -f 옵션을 사용하여 두 번째 필드(열)를 추출하도록 지정했다. 그러면 다음과 같은 출력이 생성된다: \ndeer\nrabbit\nraccoon\nrabbit\ndeer\nfox\nrabbit\nbear\nuniq 명령은 파일에 인접한 일치하는 행을 필터링하여 중복을 제거한다. 파일에 담겨 있는 동물이 무엇인지를 알아내려면, 다른 어떤 명령어가 파이프라인에 추가되어야 하나요? (동물 이름에 어떠한 중복도 없어야 합니다.)\n\n\n\n\n\n\n해답과 설명\n\n\n\n$ cut -d , -f 2 animals.csv | sort | uniq\n\n\n\n\n예제 4.7 (파이프 선택?) animals.csv 파일은 아래 형식으로 8줄로 구성되어 있다:\n2012-11-05,deer,5\n2012-11-05,rabbit,22\n2012-11-05,raccoon,7\n2012-11-06,rabbit,19\n...\nuniq 명령에 -c 옵션을 넣어 발생한 횟수를 카운트할 수 있다. shell-lesson-data/exercise-data/animal-counts 현재 디렉토리로 가정하고, 다음 중 어떤 명령어가 동물 종류별로 전체 출현 빈도수를 나타내는 표를 작성하는데 사용하면 좋을까요?\n\nsort animals.csv | uniq -c\nsort -t, -k2,2 animals.csv | uniq -c\ncut -d, -f 2 animals.csv | uniq -c\ncut -d, -f 2 animals.csv | sort | uniq -c\ncut -d, -f 2 animals.csv | sort | uniq -c | wc -l\n\n\n\n\n\n\n\n해답과 설명\n\n\n\n정답은 4.\n정답을 이해하는데 어려움이 있으면, (shell-lesson-data/exercise-data/animal-counts 디렉토리에 위치한 것을 확인한 후) 명령어 전체를 실행하거나, 파이프라인 일부를 실행해 본다.\n\n\n\n\n\n\n\n\n\n입력 방향변경\n\n\n\n프로그램의 출력 결과 방향변경을 위해서 &gt;을 사용하는 것과 마찬가지로, &lt;을 사용해서 입력을 되돌릴 수도 있다. 즉, 표준입력 대신에 파일로부터 읽어 들일 수 있다. 예를 들어, wc ammonia.pdb 와 같이 작성하는 대신에, wc &lt; ammonia.pdb 작성할 수 있다. 첫째 사례는, wc는 무슨 파일을 여는지를 명령 라인의 매개변수에서 얻는다. 두번째 사례는, wc에 명령 라인 매개변수가 없다. 그래서 표준 입력에서 읽지만, 쉘에게 ammonia.pdb의 내용을 wc에 표준 입력으로 보내라고 했다.\n\n\n\n예제 4.8 (&lt; 기호의 의미는?) 다운로드 예제 데이터를 갖고 있는 최상위 shell-lesson-data 디렉토리로 작업 디렉토리를 변경한다. 다음 두 명령어 차이는 무엇인가?\n$ wc -l notes.txt\n$ wc -l &lt; notes.txt\n\n\n\n\n\n\n해답과 설명\n\n\n\n&lt; 기호는 입력을 방향변경을 해서 명령어로 전달한다.\n상기 예제 모두에서, 쉘은 입력에서 wc 명령어를 통해 행수를 반환한다. 첫번째 예제에서, 입력은 notes.txt 파일이고, 파일명이 wc 명령어로부터 출력으로 주어지게 된다. 두번째 예제로부터, notes.txt 파일 내용이 표준입력으로 방향변경을 통해 보내지게 된다. 이것은 마치 프롬프트에서 파일 콘텐츠를 타이핑하는 것과 같다. 따라서, 파일명이 출력에 주어지지 않는다 - 단지 행번호만 주어진다. 다음과 같이 타이핑해보자:\n$ wc -l\nthis\nis\na test\nCtrl-D\nCtrl-D를 타이핑하게 되면 쉘이 입력을 마무리한 것을 알게 전달하는 역할을 한다.\n\n\n\n\n예제 4.9 (uniq가 왜 인접한 중복 행만을 단지 제거한다고 생각합니까?) 명령문 uniq는 입력으로부터 인접한 중복된 행을 제거한다. 예를 들어, salmon.txt 파일에 다음이 포함되었다면,\ncoho\ncoho\nsteelhead\ncoho\nsteelhead\nsteelhead\nshell-lesson-data/data 디렉토리의 uniq salmon.txt 명령문 실행은 다음을 출력한다.\ncoho\nsteelhead\ncoho\nsteelhead\nuniq가 왜 인접한 중복 행만을 단지 제거한다고 생각합니까? (힌트: 매우 큰 파일을 생각해보세요.) 모든 중복된 행을 제거하기 위해, 파이프로 다른 어떤 명령어를 조합할 수 있을까요?\n\n\n\n\n\n\n해답과 설명\n\n\n\n$ sort salmon.txt | uniq",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "04-shell-pipefilter.html#사례-파일-확인하기",
    "href": "04-shell-pipefilter.html#사례-파일-확인하기",
    "title": "4  파이프와 필터",
    "section": "4.7 사례: 파일 확인하기",
    "text": "4.7 사례: 파일 확인하기\n앞에서 설명한 것처럼 넬(nelle) 박사는 분석기를 통해 시료를 시험해서 north-pacific-gyre 디렉토리에 17개 파일을 생성했다. 빠르게 확인하기 위해, shell-lesson-data 디렉토리에서 다음과 같이 타이핑한다:\n$ cd north-pacific-gyre/2012-07-03\n$ wc -l *.txt\n결과는 다음과 같은 18 행이 출력된다:\n300 NENE01729A.txt\n300 NENE01729B.txt\n300 NENE01736A.txt\n300 NENE01751A.txt\n300 NENE01751B.txt\n300 NENE01812A.txt\n... ...\n이번에는 다음과 같이 타이핑한다:\n$ wc -l *.txt | sort -n | head -n 5\n\n240 NENE02018B.txt\n300 NENE01729A.txt\n300 NENE01729B.txt\n300 NENE01736A.txt\n300 NENE01751A.txt\n이런, 파일중에 하나가 다른 것보다 60행이 짧다. 다시 돌아가서 확인하면, 월요일 아침 8:00 시각에 분석을 수행한 것을 알고 있다 — 아마도 누군가 주말에 기계를 사용했고, 다시 재설정하는 것을 깜빡 잊었을 것이다. 시료를 다시 시험하기 전에 파일 중에 너무 큰 데이터가 있는지를 확인한다:\n$ wc -l *.txt | sort -n | tail -n 5\n\n 300 NENE02040B.txt\n 300 NENE02040Z.txt\n 300 NENE02043A.txt\n 300 NENE02043B.txt\n5040 total\n숫자는 예뻐 보인다 — 하지만 끝에서 세번째 줄에 ‘Z’는 무엇일까? 모든 시료는 ’A’ 혹은 ’B’로 표시되어야 한다. 시험실 관례로 ’Z’는 결측치가 있는 시료를 표식하기 위해 사용된다. 더 많은 결측 시료를 찾기 위해, 다음과 같이 타이핑한다:\n$ ls *Z.txt\n\nNENE01971Z.txt    NENE02040Z.txt\n노트북의 로그 이력을 확인할 때, 상기 샘플 각각에 대해 깊이(depth) 정보에 대해서 기록된 것이 없었다. 다른 방법으로 정보를 더 수집하기에는 너무 늦어서, 분석에서 두 파일을 제외하기로 했다. rm 명령어를 사용하여 삭제할 수 있지만, 향후에 깊이(depth)정보가 관련없는 다른 분석을 실시할 수도 있다. 대신 나중에 와일드카드 표현식 NENE*A.txt NENE*B.txt를 사용하여 파일을 선정하는데 주의를 기울여야된다.\n\n예제 4.10 (불필요한 파일 제거) 저장공간을 절약하고자 중간 처리된 데이터 파일을 삭제하고 원본 파일과 처리 스크립트만 보관했으면 한다고 가정하자.\n원본 파일은 .dat으로 끝나고, 처리된 파일은 .txt으로 끝난다. 다음 중 어떤 명령어가 처리과정에서 생긴 중간 모든 파일을 삭제하게 하는가?\n\nrm ?.txt\nrm *.txt\nrm * .txt\nrm *.*\n\n\n\n\n\n\n\n정답과 설명\n\n\n\n\n한문자 .txt 파일을 제거한다.\n정답\n\n기호로 인해 현재 디렉토리 모든 파일과 디렉토리를 매칭시킨다. 그래서 * 기호로 매칭되는 모든 것과 추가로 .txt 파일도 삭제한다.\n\n. 기호는 임의 확장자를 갖는 모든 파일을 매칭시킨다. 따라서 . 기호는 모든 파일을 삭제한다.\n\n\n\n\n\n예제 4.11 (와일드카드 표현식) 와일드카드 표현식(Wildcard Expressions)은 매우 복잡할 수 있지만, 종종 다소 장황할 수 있는 비용을 지불하고 간단한 구문만 사용해서 작성하기도 한다.\n/shell-lesson-data/north-pacific-gyre 디렉토리를 생각해 보자: *[AB].txt 와일드카드 표현식은 A.txt 혹은 B.txt으로 끝나는 모든 파일을 매칭시킨다. 이 와일드카드 표현식을 잊었다고 상상해보자:\n\n[] 구문을 사용하지 않는 기본 와일드드카드 표현식으로 동일하게 파일을 매칭할 수 있을까? 힌트: 표현식이 하나 이상 필요할 수도 있다.\n[] 구문을 사용하지 않고 작성한 표현식은 동일한 파일을 매칭한다. 두 출력결과의 작은 차이점은 무엇인가?\n최초 와일드카드 표현식은 오류가 나지 않는데 어떤 상황에서 본인 표현식은 오류 메시지를 출력하는가?\n\n\n\n\n\n\n\n해답과 설명\n\n\n\n\n\n\n$ ls *A.txt\n$ ls *B.txt\n\n새로운 명령어에서 나온 출력결과는 명령어가 두개라 구분된다.\nA.txt로 끝나는 파일이 없거나 B.txt로 끝나는 파일이 없는 경우 그렇다.\n\n\n\n\nPDB (Protein Data Bank)는 큰 생물학적 분자, 예를 들면 단백질이나 핵산의 3차원 구조 데이터를 저장한 데이터베이스다. 이 데이터는 보통 X-선 결정학, NMR 분광학, 혹은 점점 더 많이 사용되는 cryo-전자 현미경을 통해 얻어지며, PDB 웹사이트를 통해 무료로 이용할 수 있다. 파일 확장자는 .pdb 를 갖고 예를 들어 “methane.pdb”라는 파일은 메테인이라는 화합물의 3차원 구조 정보를 담고 있는 PDB (Protein Data Bank) 파일을 지칭합하고 메테인 분자의 원자들의 위치, 분자의 종류 등의 정보를 포함하고 있을 것이다.\n\n행수가 가장 많은 파일 찾기\nshell-lesson-data/exercise-data/ 디렉토리에 있는 모든 *.pdb 파일을 찾아 각 파일의 행수를 구한 후 가장 행수가 많은 파일부터 역순으로 정렬하는 유닉스 쉘 명령어를 작성해보자.\n\n프롬프트: shell-lesson-data/exercise-data/ 디렉토리로 이동한 후 확 장자가 .pdb인 파일 행수를 구한 후에 파일별 행수를 내림차순으로 출력하도록 하세요.\n\n얻어진 유닉스 쉘 명령어를 실행하게 되면 앞서 순차적으로 작성한 유닉스 명령어와 동일한 결과를 얻게 된다.\n$ sgpt -s \"shell-lesson-data/exercise-data/ 디렉토리로 이동한 후 확\n장자가 .pdb인 파일 행수를 구한 후에 파일별 행수를 내림차순으로 출력하도록 하세요.\"\n\ncd shell-lesson-data/exercise-data/ && find . -name \"*.pdb\" | xargs wc -l | sort -nr\n[E]xecute, [D]escribe, [A]bort: D\nThis command changes the current directory to \"shell-lesson-data/exercise-data/\", then finds all files with the extension \".pdb\" in that directory and its subdirectories, counts the number of lines in each file using \"wc -l\", and finally sorts the results in descending order.\n\n[E]xecute, [D]escribe, [A]bort: E\n 107 total\n  30 ./alkanes/octane.pdb\n  21 ./alkanes/pentane.pdb\n  20 ./alkanes/cubane.pdb\n  15 ./alkanes/propane.pdb\n  12 ./alkanes/ethane.pdb\n   9 ./alkanes/methane.pdb\n챗GPT가 프롬프트로 작성한 유닉스 쉘 명령어를 해석하면 다음과 같다.\n\n현재 디렉터리를 “shell-lesson-data/exercise-data/”로 변경한 다음 해당 디렉터리 및 하위 디렉터리에서 확장자가 “.pdb”인 모든 파일을 찾고, “wc -l”을 사용하여 각 파일의 줄 수를 세고, 마지막으로 결과를 내림차순으로 정렬한다.\n\n바로 실행시키게 되면 앞서 실행한 것과 역순으로 정렬된 것만 반대로 차이가 있고 결과는 동일함을 알 수 있다.\n\n\nanimals.csv 관측동물 빈도수\nshell-lesson-data/exercise-data/animal-counts 디렉토리에 있는 animals.csv은 다음과 같은 형태로 되어 있다. 즉, 콤마로 구분되는 3개의 칼럼과 8개의 관측점으로 구성되어 있지만 .csv 파일에 칼럼명이 없는 구조를 갖고 있다. animals.csv에 종별 관측빈도를 구해보자. 챗GPT 프롬프트를 이에 맞춰 작성하고 sgpt -s로 실행해서 결과를 얻어보자.\n2012-11-05,deer,5\n2012-11-05,rabbit,22\n2012-11-05,raccoon,7\n2012-11-06,rabbit,19\n2012-11-06,deer,2\n2012-11-06,fox,4\n2012-11-07,rabbit,16\n2012-11-07,bear,1\n\n프롬프트: shell-lesson-data/exercise-data/animal-counts 디렉토리로 이동한 후 animals.csv 파일에서 두번째 칼럼에서 관측된 동물의 빈도수를 역순으로 구해서 출력하시요\n\n$ sgpt -s \"shell-lesson-data/exercise-data/animal-counts 디렉토리로 이동한 후 animals.csv 파일에서 두번째 칼럼에서 관측된 동물의 빈도수를 역순으로 구해서 출력하시요\"\ncd shell-lesson-data/exercise-data/animal-counts && awk -F',' '{print $2}' animals.csv | sort | uniq -c | sort -nr\n[E]xecute, [D]escribe, [A]bort: D\nThis command changes the directory to \"shell-lesson-data/exercise-data/animal-counts\" and then uses awk to print the second column of the file \"animals.csv\", sorts the output, counts the unique values, and sorts them in descending order.\n[E]xecute, [D]escribe, [A]bort: E\n      3 rabbit\n      2 deer\n      1 raccoon\n      1 fox\n      1 bear\n챗GPT가 프롬프트로 작성한 유닉스 쉘 명령어를 해석하면 다음과 같다.\n\n디렉터리를 “shell-lesson-data/exercise-data/animal-counts”로 변경한 다음 awk를 사용하여 파일 “animals.csv”의 두 번째 열을 인쇄하고 출력결과를 정렬하고 유일무이한 고유값을 계산한 다음 내림차순으로 정렬한다.\n\n얻어진 유닉스 쉘 명령어를 실행하게 되면 토끼(rabbit)가 3회 등 정확한 계산결과가 도출된다.",
    "crumbs": [
      "쉘 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>파이프와 필터</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "AbdulMajedRaja. 2020. “Penguins Dataset Overview - Iris\nAlternative in r Using palmerpenguins.” Programming with\nR. https://www.programmingwithr.com/penguins-dataset-overview-iris-alternative-in-r/.\n\n\nBach, Maurice J. 1986. “The Design of the UNIX.” RTM.\nOperating System Prentice Hall, 312–29.\n\n\nBass, Len, Paul Clements, and Rick Kazman. 2003. Software\nArchitecture in Practice. Addison-Wesley Professional.\n\n\nBoettiger, Carl. 2015. “An Introduction to Docker for Reproducible\nResearch.” ACM SIGOPS Operating Systems Review 49 (1):\n71–79.\n\n\nEdwards, Anthony W Fisher. 2000. “The Genetical Theory of Natural\nSelection.” Genetics 154 (4): 1419–26.\n\n\nGarlan, David, and Mary Shaw. 1993. “An Introduction to Software\nArchitecture.” In Advances in Software Engineering and\nKnowledge Engineering, 1–39. World Scientific.\n\n\nGozalo-Brizuela, Roberto, and Eduardo C Garrido-Merchan. 2023.\n“ChatGPT Is Not All You Need. A State of the Art Review of Large\nGenerative AI Models.” arXiv Preprint arXiv:2301.04655.\n\n\nJanssens, Jeroen. 2021. Data Science at the Command Line. \"\nO’Reilly Media, Inc.\".\n\n\nKB, Gorman, Williams TD, and Fraser WR. 2014. “Ecological Sexual\nDimorphism and Environmental Variability Within a Community of Antarctic\nPenguins (Genus Pygoscelis).” PLoS ONE 9(3) (e90081):\n–13. https://doi.org/10.1371/journal.pone.0090081.\n\n\nLevy, Ido. 2019. “Eugenics and the Ethics of Statistical\nAnalysis.” GEORGETOWN PUBLIC POLICY REVIEW. https://gppreview.com/2019/12/16/eugenics-ethics-statistical-analysis/.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).” The\nAmerican Statistician 72 (1): 80–88.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSeverance, Charles. 2009. Python for Informatics Exploring\nInformation.\n\n\nSeverance, Charles Russell. 2015. Python for Informatics: Exploring\nInformation. Translated by Kwangchun Lee. Korean Edition.\nCreateSpace Independent Publishing Platform. http://www.pythonlearn.com.\n\n\nTolle, Kristin M, D Stewart W Tansley, and Anthony JG Hey. 2011.\n“The Fourth Paradigm: Data-Intensive Scientific Discovery [Point\nof View].” Proceedings of the IEEE 99 (8): 1334–37.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "참고문헌"
    ]
  }
]