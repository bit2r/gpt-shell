[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gpt-shell",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” https://arxiv.org/abs/1706.03762."
  },
  {
    "objectID": "00_setup.html",
    "href": "00_setup.html",
    "title": "1  환경설정",
    "section": "",
    "text": "2 OpenAI 모형\nOpenAI에서 제공하는 다양한 모델을 확인할 수 있다. system이 소유한 GPT 모형을 살펴보자.\n프로그램 코드(code) 관련된 GPT 모형도 확인할 수 있다."
  },
  {
    "objectID": "00_setup.html#가상환경-설정",
    "href": "00_setup.html#가상환경-설정",
    "title": "1  환경설정",
    "section": "1.1 가상환경 설정",
    "text": "1.1 가상환경 설정\n다양한 가상환경이 있어 필요한 패키지를 사용하여 파이썬 가상환경을 구축한다. 파이썬 3.3 버전부터 내장된 venv, 많이 사용되는 virtualenvwrapper, virtualenv 등이 유명하다. 본인 취향에 맞는 가상환경을 특정하여 업무에 사용한다. 다음은 venv를 사용해서 가상 개발환경을 구축하는 것을 예시로 보여주고 있다.\n## 디렉토리 생성 및 프로젝트 디렉토리 이동\nmkdir myproject\ncd myproject\n\n## 가상환경 생성\npython -m venv myenv\n\n## 가상환경 활성화\nmyenv\\Scripts\\activate # 윈도우즈\nsource myenv/bin/activate # 리눅스/맥\n\n## 가상환경 비활성화\ndeactivate"
  },
  {
    "objectID": "00_setup.html#api-key-얻기",
    "href": "00_setup.html#api-key-얻기",
    "title": "1  환경설정",
    "section": "1.2 API KEY 얻기",
    "text": "1.2 API KEY 얻기\n가상환경을 구축한 다음 OpenAI에서 제공하는 공식 API에 접근할 수 있는 API 키를 생성하는 것이다. https://openai.com/api/ 1 로 이동하여 계정을 만듭니다.\n안내에 따라 계정을 생성한 다음 https://platform.openai.com/account/api-keys 2 로 이동하여 API 키를 생성한다.\nAPI 키는 조직에 속해야 하며, 조직을 생성하라는 메시지가 표시되는 경우 조직명을 입력한다. 하나의 조직에 속한 경우 조직 ID(Organization ID)를 별도 생성할 필요는 없다. OpenAI 계정을 통해서는 생성한 API KEY는 다시 볼 수 없기 때문에 생성한 비밀 키를 안전하고 접근하기 쉬운 곳에 저장한다."
  },
  {
    "objectID": "00_setup.html#api-key-저장",
    "href": "00_setup.html#api-key-저장",
    "title": "1  환경설정",
    "section": "1.3 API KEY 저장",
    "text": "1.3 API KEY 저장\nAPI KEY를 환경변수로 지정하여 호출하는 방식도 있고, 작업 프로젝트 디렉토리에 로컬 파일에 저장하여 사용하는 방식도 있다. 먼저 윈도우에서 시스템으로 들어가서 환경 변수로 지정하면 해당 변수(OPENAI_API_KEY)를 다양한 프로그램에서 호출하여 사용할 수 있다.\n\n다른 방식은 .env와 같은 파일을 프로젝트 디렉토리 아래 숨긴 파일에 지정하여 사용하는 방식이다. 이런 경우 .gitignore 파일에 버전제어 대상에서 제외시켜 두는 것을 필히 기억한다."
  },
  {
    "objectID": "00_setup.html#헬로월드",
    "href": "00_setup.html#헬로월드",
    "title": "1  환경설정",
    "section": "1.4 헬로월드",
    "text": "1.4 헬로월드\nOpenAI API KEY도 준비가 되었으면 헬로월드 프로그램을 작성해보자. 개발자가 하나의 조직에 속한 경우, API KEY를 운영체제 환경변수로 지정한 경우 다음과 같이 시스템 환경에서 OPENAI_API_KEY 키를 가져와서 OpenAI에서 제공하는 모델목록을 확인할 수 있다.\n\nimport os\nimport openai\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# API 호출 및 모델 목록 출력\nmodels = openai.Model.list()\nprint(models['data'][0])\n\n{\n  \"created\": 1649358449,\n  \"id\": \"babbage\",\n  \"object\": \"model\",\n  \"owned_by\": \"openai\",\n  \"parent\": null,\n  \"permission\": [\n    {\n      \"allow_create_engine\": false,\n      \"allow_fine_tuning\": false,\n      \"allow_logprobs\": true,\n      \"allow_sampling\": true,\n      \"allow_search_indices\": false,\n      \"allow_view\": true,\n      \"created\": 1669085501,\n      \"group\": null,\n      \"id\": \"modelperm-49FUp5v084tBB49tC4z8LPH5\",\n      \"is_blocking\": false,\n      \"object\": \"model_permission\",\n      \"organization\": \"*\"\n    }\n  ],\n  \"root\": \"babbage\"\n}\n\n\n다른 방식은 로컬 파일에 API KEY와 ORG ID 를 저장하고 이를 불러와서 개발에 사용하는 방식이다.\n\nimport os\nimport openai\n\n# .env 파일에서 API_KEY 와 ORG_ID 을 읽어온다.\nwith open(\".env\") as lines:\n  for line in lines:\n    key, value = line.strip().split(\"=\")\n    os.environ[key] = value\n    \n# api_key와 organization 지정\nopenai.api_key = os.environ.get(\"API_KEY\")\nopenai.organization = os.environ.get(\"ORG_ID\")\n\n# API 호출 및 모델 목록 출력\ngpt_models = openai.Model.list()\n\nprint(gpt_models['data'][0])\n\n{\n  \"created\": 1649358449,\n  \"id\": \"babbage\",\n  \"object\": \"model\",\n  \"owned_by\": \"openai\",\n  \"parent\": null,\n  \"permission\": [\n    {\n      \"allow_create_engine\": false,\n      \"allow_fine_tuning\": false,\n      \"allow_logprobs\": true,\n      \"allow_sampling\": true,\n      \"allow_search_indices\": false,\n      \"allow_view\": true,\n      \"created\": 1669085501,\n      \"group\": null,\n      \"id\": \"modelperm-49FUp5v084tBB49tC4z8LPH5\",\n      \"is_blocking\": false,\n      \"object\": \"model_permission\",\n      \"organization\": \"*\"\n    }\n  ],\n  \"root\": \"babbage\"\n}"
  },
  {
    "objectID": "01_openAI.html#텍스트-완성",
    "href": "01_openAI.html#텍스트-완성",
    "title": "2  OpenAI 들어가며",
    "section": "2.1 텍스트 완성",
    "text": "2.1 텍스트 완성\nGPT를 사용하여 다양한 작업을 수행할 수 있지만 가장 기본적인 작업은 글쓰기다. GPT가 생성형 AI로 해당 텍스트를 주어지면 나머지 텍스트를 해당 최대 토큰 크기(max_tokens) 길이만큼 텍스트를 생성해준다.\n\nimport os\nimport openai\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ncomplete_next = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"나의 살던 고향은\",\n  max_tokens=7,\n  temperature=0)\n  \ncomplete_next['choices'][0]['text']\n\n'\\n\\n나의'\n\n\n토큰 크기를 100으로 지정하면 제법 긴 텍스트를 출력한다. 영어 토큰에 최적화되어 있는 관계로 한글의 경우 토큰 낭비(?)가 심한 것으로 보인다. 고로 비용이 제법 나가는 점은 한국어로 작업을 할 때 고려해야만 된다.\n\ncomplete_next_100 = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"나의 살던 고향은\",\n  max_tokens=100,\n  temperature=0)\n  \ncomplete_next_100['choices'][0]['text']\n\n'\\n\\n나의 고향은 전라도 익산입니다. 익산은 전라도의 중부에 위치한 도시로, 전라도의 중'"
  },
  {
    "objectID": "01_openAI.html#키워드-추출",
    "href": "01_openAI.html#키워드-추출",
    "title": "2  OpenAI 들어가며",
    "section": "2.2 키워드 추출",
    "text": "2.2 키워드 추출\n조금더 흥미로운 주제로 해당 문서를 제시하고 관련 텍스트의 주요 키워드를 추출해보자. Attention Is All You Need 논문(Vaswani et al. 2017)은 AI 분야에서 획기적인 논문으로 평가받있지만 별도 키워드는 제시되고 있지 않아 논문 초록을 앞에 제시하고 Keywords:를 뒤에 두고 논문의 주요 키워드를 추출하게 한다.\n\nprompt_keywords = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n keywords:\"\n\nkeywords = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=prompt_keywords,\n  temperature = 0.5,\n  max_tokens  = 50)\n\nkeywords['choices'][0]['text']\n\nmax_tokens을 50으로 제한하여 temperature = 0.5로 너무 창의적이지 않게 키워드 추출 작업을 지시한 경우 다음과 같은 결과를 어덱 된다.\n'\\nSequence transduction, neural networks, attention mechanisms, machine translation, parsing'\n이번에는 한글 논문초록에서 키워드를 추출해보자. 2020년 출간된 논문(lee2020?)의 한글 초록에서 제시된 키워드와 OpenAI GPT가 제시하고 있는 키워드와 비교해보자.\n\n논문 소스코드: 바로가기\nPDF 출판 논문: 다운로드\n\n\nprompt_keywords = \"알파고가 2016년 바둑 인간 챔피언 이세돌 9단을 현격한 기량차이로 격파하면서 인공지능에 대한 관심이 급격히 증가하였다. 그와 동시에 기계가 인간의 일자리 잠식을 가속화하면서 막연한 불안감이 삽시간에 전파되었다. 기계와의 일자리 경쟁은 컴퓨터의 출현이전부터 시작되었지만 인간만의 고유한 영역으로 알고 있던 인지, 창작 등 다양한 분야에서 오히려 인간보다 더 우수한 성능과 저렴한 가격 경쟁력을 보여주면서 기존 인간의 일자리가 기계에 대체되는 것이 가시권에 들었다. 이번 문헌조사와 실증 데이터 분석을 통해서 기계가 인간의 일자리를 대체하는 자동화의 본질에 대해서 살펴보고, 인간과 기계의 업무 분장을 통해 더 생산성을 높일 수 있는 방안을 제시하고자 한다.\\n\\n 키워드:\"\n\nkeywords = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=prompt_keywords,\n  temperature = 0.5,\n  max_tokens  = 100)\n\nkeywords['choices'][0]['text']\n\n' 인공지능, 자동화, 인간과 기계의 업무 분장, 생산성 \\n\\n이 문헌조사는 인공지능이 인'\nGPT가 생성한 키워드를 논문저자가 추출한 키워드와 비교하면 다소 차이가 있지만 그래도 상위 3개 키워드는 높은 일치도를 보이고 있다.\n\n\n\nOpenAI GPT 키워드\n\n인공지능\n자동화\n인간과 기계의 업무 분장\n생산성 문헌조사는 인공지능이 인’\n\n\n\n\n논문저자 추출\n\n자동화\n데이터 과학\n인공지능\n일자리\n기계와 사람의 업무분장\n\n\n\n\nGPT-4는 더 높은 성능을 보여주고 있다. https://chat.openai.com/chat?model=gpt-4에 해당 텍스트를 던져주면 다음과 같이 키워드를 추출하고 요약을 해준다."
  },
  {
    "objectID": "01_openAI.html#텍스트-요약",
    "href": "01_openAI.html#텍스트-요약",
    "title": "2  OpenAI 들어가며",
    "section": "2.3 텍스트 요약",
    "text": "2.3 텍스트 요약\nAttention Is All You Need 논문(Vaswani et al. 2017) 초록은 https://platform.openai.com/tokenizer 계산기를 통해 230개 토큰 1,138 문자로 작성된 것이 확인된다. 영어 기준 다음과 같은 맥락을 이해하고 이를 대략 20% 수준 50 토큰으로 줄여보자.\n\n100 토큰은 대략 75 단어\n평균 단어는 대략 5 문자로 구성\n100 토큰은 375개 문자\n\n\nprompt_keywords = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n summary:\"\n\nkeywords = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=prompt_keywords,\n  temperature = 0.5,\n  max_tokens  = 50)\n\nkeywords['choices'][0]['text']\n\n상기 논문 초록을 50개 토큰으로 요약하면 다음과 같이 48개 토큰, 277 문자수로 요약해준다.\n'\\n\\nThe Transformer is a new neural network architecture based solely on attention mechanisms, which is shown to outperform existing models on two machine translation tasks. It is more parallelizable and requires significantly less time to train than existing models, achieving a B'"
  },
  {
    "objectID": "01_openAI.html#여론조사-할일-생성",
    "href": "01_openAI.html#여론조사-할일-생성",
    "title": "2  OpenAI 들어가며",
    "section": "2.4 여론조사 할일 생성",
    "text": "2.4 여론조사 할일 생성\nTO-DO 리스트를 제작하는 것은 해당 작업을 절차적으로 구분지어 수행할 수 있게 되어 해당 작업의 성공가능성을 높이고 생산성도 높일 수 있다. 여론조사를 한사람이 수행하는 경우는 거의 없지만 일반적으로 여론조사에서 수행할 일에 대해서 지시명령어를 작성하여 결과를 살펴보자.\n\ntodo_list = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"여론조사를 위해서 해야될 일을 작성하세요\\n\\n1.\",\n  temperature=0.3,\n  max_tokens = 1000,\n  top_p = 0.1,\n  frequency_penalty=0,\n  presence_penalty=0.5,\n  stop=[\"6.\"]\n)\n\ntodo_text = todo_list['choices'][0]['text']\n\n' 여론조사 대상자를 선정하고, 여론조사 대상자의 수를 결정합니다.\\n\\n2. 여론조사 대상자들에게 여론조사 질문지를 배포합니다.\\n\\n3. 여론조사 대상자들에게 여론조사 응답을 요청합니다.\\n\\n4. 여론조사 응답을 수집하고, 분석합니다.\\n\\n5. 여론조사 결과를 보고서로 작성합니다.'\n\nlibrary(reticulate)\ncat(glue::glue(\"1. {py$todo_text}\"))\n\n작업수행결과를 가독성 좋게 정리하면 다음과 같다.\n\n여론조사 대상자를 선정하고, 여론조사 대상자의 수를 결정합니다.\n여론조사 대상자들에게 여론조사 질문지를 배포합니다.\n여론조사 대상자들에게 여론조사 응답을 요청합니다.\n여론조사 응답을 수집하고, 분석합니다.\n여론조사 결과를 보고서로 작성합니다.\n\n지금까지 OpenAI API를 사용하여 텍스트 자동생성기능을 활용하여 키워드 추출, 문서요약, 작업목록 생성과 같은 업무를 통해 가능성을 살펴봤다. 이제 데이터 과학업무 생산성의 주요한 도구인 유닉스 쉘(Unix Shell)을 챗GPT로 또 다른 데이터 과학의 세계로 나아가자.\n\n\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” https://arxiv.org/abs/1706.03762."
  },
  {
    "objectID": "shell-intro.html#shell-background",
    "href": "shell-intro.html#shell-background",
    "title": "3  쉘(Shell) 소개",
    "section": "3.1 배경",
    "text": "3.1 배경\n상위 수준에서 컴퓨터는 네가지 일을 수행한다:\n\n프로그램 실행\n데이터 저장\n컴퓨터간 상호 의사소통\n사람과 상호작용\n\n마지막 작업을 뇌-컴퓨터 연결, 음성 인터페이스를 포함한 다양한 많은 방식으로 수행하고 있지만 아직은 초보적인 수준이어서, 대부분은 WIMP((Window) 윈도우, (Icon)아이콘, (Mouse)마우스, (Pointer)포인터)를 사용한다. 1980년대까지 이러한 기술은 보편적이지 않았지만, 기술의 뿌리는 1960년대 Doug Engelbart의 작업에 있고, “The Mother of All Demos”로 불리는 것에서 볼 수 있다.\n조금 더 멀리 거슬러 올라가면, 초기 컴퓨터와 상호작용하는 유일한 방법은 와이어로 다시 연결하는 것이다. 하지만, 중간에 1950년에서 1980년 사이 대부분의 사람들이 라인 프린터(line printer)를 사용했다. 이런 장치는 표준 키보드에 있는 문자, 숫자, 특수부호의 입력과 출력만 허용해서, 프로그래밍 언어와 인터페이스는 이러한 제약사항에서 설계됐다.\n여전히 전통적인 화면, 마우수, 터치패드, 키보드를 사용하지만 터치 인터페이스와 음성 인터페이스가 보편화되고 있다.\n이런 종류의 인터페이스를 지금 대부분의 사람들이 사용하는 그래픽 사용자 인터페이스(GUI, graphical user interface)과 구별하기 위해서 명령-라인 인터페이스(CLI, command-line interface)라고 한다. CLI의 핵심은 읽기-평가-출력(REPL,read-evaluate-print loop)이다: 사용자가 명령어를 타이핑하고 엔터(enter)/반환(return)키를 입력하면, 컴퓨터가 읽고, 실행하고, 결과를 출력한다. 그러고 나면, 사용자는 다른 명령를 타이핑하는 것을 로그 오프해서 시스템을 빠져 나갈때까지 계속한다.\nGUI는 WIMP((Window) 윈도우, (Icon)아이콘, (Mouse)마우스, (Pointer)포인터)로 구성되는데 배우기 쉽고, 단순 작업에 대해서는 환상적이다. “클릭”하게 되면 명령이 “내가 원하는 작업을 수행해”라고 손쉽게 컴퓨터에 통역된다. 하지만, 이런 마술은 단순한 작업을 수행하고, 정확하게 이러한 유형의 작업을 수행할 수 있는 프로그램에 불과하다.\n만약 복잡하고, 특정 목적에 부합되는 훨씬 묵직한 작업을 컴퓨터에 내리고자 한다고 해서, 난해하거나 어렵거나할 필요는 없고, 단지 명령 어휘가 필요하고 이를 사용하는데 필요한 단순한 문법만 필요로 한다.\n쉘이 이런 기능을 제공한다 - 단순한 언어로 이를 사용하는데 명령-라인 인터페이스가 필요하다. 명령라인 인터페이스의 심장은 읽기-평가-출력(REPL,read-evaluate-print loop)이다. REPL로 불리는 이유는 쉘에 명령어를 타이핑하고 Return를 치게되면 컴퓨터가 명령어를 읽어들이고 나서, 평가(혹은 실행)하고 출력결과를 화면에 뿌린다. 또 다른 명령어를 입력할 때까지 대기하는 루푸를 반복하게 되서 그렇다.\n상기 묘사가 마치 사용자가 직접 명령어를 컴퓨터에 보내고, 컴퓨터는 사용자에게 직접적으로 출력을 보내는 것처럼 들린다. 사실 중간에 명령 쉘(command shell)로 불리는 프로그램이 있다. 사용자가 타이핑하는 것은 쉘로 간다. 쉘은 무슨 명령어를 수행할지 파악해서 컴퓨터에게 수행하도록 지시한다. 쉘을 조개 껍데기(shell)로 불리는데 이유는 운영체제를 감싸서, 복잡성 일부를 숨겨서 운영체제와 더 단순하게 상호작용하게 만든다."
  },
  {
    "objectID": "shell-intro.html#shell-shell",
    "href": "shell-intro.html#shell-shell",
    "title": "3  쉘(Shell) 소개",
    "section": "3.2 쉘(Shell)",
    "text": "3.2 쉘(Shell)\n쉘(Shell)은 다른 것과 마찬가지로 프로그램이다. 조금 특별한 것은 자신이 연산을 수행하기 보다 다른 프로그램을 실행한다는 것이다. 가장 보편적인 유닉스 쉘(Unix Shell)은 Bash(Bourne Again SHell)다. Stephen Bourne이 작성한 쉘에서 나와서 그렇게 불리우고 — 프로그래머 사이에 재치로 통한다. Bash는 대부분의 유닉스 컴퓨터에 기본으로 장착되는 쉘이고, 윈도우용으로 유닉스스런 도구로 제공되는 패키지 대부분에도 적용된다.\nBash나 다른 쉘을 사용하는 것이 마우스를 사용하는 것보다 프로그래밍 작성하는 느낌이 난다. 명령어는 간략해서 (흔히 단지 2~3자리 문자다), 명령어는 자주 암호스럽고, 출력은 그래프같이 시각적인 것보다 텍스트줄로 쭉 뿌려진다. 다른 한편으로, 쉘을 사용하여 좀더 강력한 방식으로 현존하는 도구를 단지 키보드 입력값 몇개를 조합해서 대용량의 데이터를 자동적으로 처리할 수 있는 파이프라인을 구축할 수 있게 한다. 추가로, 명령 라인은 종종 멀리 떨어진 컴퓨터 혹은 슈퍼컴퓨터와 상호작용하는 가장 쉬운 방법이다. 고성능 컴퓨팅 시스템에 포함된 다양한 특화된 도구와 자원을 실행하는데 쉘과 친숙성이 거의 필연적이다. 클러스트 컴퓨팅과 클라우드 컴퓨팅이 과학 데이터 클런칭(scientific data cruching)이 점점 대중화됨에 따라 원격 컴퓨터를 구동하는 것이 필수적인 기술이 되어가고 있다. 여기서 다뤄지는 명령-라인 기술에 기반해서 광범위한 과학적 질문과 컴퓨터적 도전과제를 처리할 수 있다."
  },
  {
    "objectID": "shell-intro.html#shell-looks-like",
    "href": "shell-intro.html#shell-looks-like",
    "title": "3  쉘(Shell) 소개",
    "section": "3.3 어떻게 생겼을까?",
    "text": "3.3 어떻게 생겼을까?\n전형적인 쉘 윈도우는 다음과 같다:\nbash-3.2$ \nbash-3.2$ ls -F / \nApplications/         System/\nLibrary/              Users/\nNetwork/              Volumes/\nbash-3.2$ \n첫번째 줄은 프롬프트(prompt)만 보여주고 있고, 쉘이 입력준비가 되었다는 것을 나타낸다. 프롬프트로 다른 텍스트를 지정할 수도 있다. 가장 중요한 것: 명령어를 타이핑할 때, 프롬프트를 타이핑하지 말고, 인식되거나 수행할 수 있는 명령어만 타이핑한다.\n예제 두번째 줄에서 타이핑한 ls -F / 부분이 전형적인 구조를 보여주고 있다: 명령어(command), 플래그(flags) (선택옵션(options) 혹은 스위치(switches)) 그리고 인자(argument). 플래그는 대쉬(-) 혹은 더블 대쉬(--)로 시작하는데 명령어의 행동에 변화를 준다.\n인자는 명령어에 작업할 대상을 일러준다(예를 들어, 파일명과 디렉토리). 종종 플래그를 매개변수(parameter)라고도 부른다. 명령어를 플래그 한개 이상, 인자도 한개 이상 사용하기도 한다: 하지만, 명령어가 항상 인자 혹은 플래그를 요구하지는 않는다.\n상기 예제의 두번째 줄에서, 명령어는 ls, 플래그는 -F, 인자는 /이 된다. 각각은 공백으로 뚜렸하게 구분된다: 만약 ls 와 -F 사이 공백을 빼먹게 되면 쉘은 ls-F 명령어를 찾게 되는데, 존재하지 않는 명령어다. 또한, 대문자도 문제가 될 수 있다: LS 명령어와 ls 명령어는 다르다.\n다음으로 명령어가 생성한 출력결과를 살펴보자. 이번 경우에 / 폴더에 위치한 파일 목록을 출력하고 있다. 금일 해당 출력결과가 무엇을 의미하는지 다룰 예정이다. 맥OS를 사용하시는 참석자분들은 이번 출력결과를 이미 인지하고 있을지도 모른다.\n마지막으로, 쉘은 프롬프트를 출력하고 다음 명령어가 타이핑되도록 대기모드로 바뀐다.\n이번 학습예제에서 프롬프트가 $이 된다. 명령어를 PS1='$ ' 타이핑하게 되면 동일하게 프롬프트를 맞출 수 있다. 하지만, 본인 취향에 맞추어 프롬프트를 둘 수도 있다 - 흔히 프롬프트에 사용자명과 디렉토리 현재 위치정보를 포함하기도 하다.\n쉘 윈도우를 열고, ls -F / 명령어를 직접 타이핑한다. (공백과 대문자가 중요함으로 잊지 말자.) 원하는 경우 프롬프트도 변경해도 좋다."
  },
  {
    "objectID": "shell-intro.html#shell-ls",
    "href": "shell-intro.html#shell-ls",
    "title": "3  쉘(Shell) 소개",
    "section": "3.4 ls 와 플래그 의미 파악",
    "text": "3.4 ls 와 플래그 의미 파악\n모든 쉘 명령어는 컴퓨터 어딘가에 저장된 프로그램으로, 쉘은 명령어를 검색해서 찾을 장소를 목록으로 이미 가지고 있다. (명령목록은 PATH로 불리는 변수(variable)에 기록되어 있지만, 이 개념을 나중에 다룰 것이라 현재로서는 그다지 중요하지는 않다.) 명령어, 플래그, 인자가 공백으로 구분된다는 점을 다시 상기하자.\nREPL(읽기-평가-출력(read-evaluate-print) 루프)를 좀더 살펴보자. “평가(evaluate)” 단계는 두가지 부분으로 구성됨에 주목한다:\n\n타이핑한 것을 읽어들인다(이번 예제에서 ls -F /) 쉘은 공백을 사용해서 명령어로 입력된 것을 명령어, 플래그, 인자로 쪼갠다.\n평가(Evaluate):\n\nls 라는 프로그램을 찾는다.\n\n찾은 프로그램을 실행하고 프로그램이 인식하고 해석한 플래그와 인자를 전달한다.\n\n프로그램 실행 결과를 출력한다.\n\n그리고 나서, 프롬프트를 출력하고 또다른 명령어를 입력받도록 대기한다.\n\n\n\n\n\n\nCommand not found 오류\n\n\n\n쉘이 타이핑한 명령어 이름을 갖는 프로그램을 찾을 수 없는 경우, 다음과 같은 오류 메시지가 출력된다:\n$ ls-F\n-bash: ls-F: command not found\n일반적으로 명령어를 잘못 타이핑했다는 의미가 된다 - 이 경우, ls 와 -F 사이 공백을 빼먹어서 그렇다. 즉, ls -F와 같이 명령을 전달하면 의도한 바가 기계에 정확히 전달된다."
  },
  {
    "objectID": "shell-intro.html#shell-difficulty",
    "href": "shell-intro.html#shell-difficulty",
    "title": "3  쉘(Shell) 소개",
    "section": "3.5 어려운가요?",
    "text": "3.5 어려운가요?\nGUI와 비교하여 컴퓨터와 상호작용하는데 있어 어려운 모형이고 학습하는데 노력과 시간이 다소 소요된다. GUI는 선택지를 보여주고, 사용자가 선택지중에서 선택하는 하는 것이다. 명령라인 인터페이스(CLI)로 선택지가 명령어와 패러미터의 조합으로 표현된다. 사용자에게 제시되는 것이 아니라서 새로운 언어의 어휘를 학습하듯이 일부 학습이 필요하다. 명령어의 일부만 배우게 되면 정말 도움이 많이 되고, 핵심적인 명령어를 다뤄보자."
  },
  {
    "objectID": "shell-intro.html#shell-flexibility",
    "href": "shell-intro.html#shell-flexibility",
    "title": "3  쉘(Shell) 소개",
    "section": "3.6 유연성과 자동화",
    "text": "3.6 유연성과 자동화\n쉘문법(Grammar of Shell)은 기존 도구를 조합해서 강력한 파이프라인을 구축하도록 해서 방대한 데이터를 자동화하여 다룰 수 있다. 명령 순서는 스크립트(script)로 작성하여 작업흐름의 재현가능성을 향상시켜서 쉽게 반복이 가능하도록 한다.\n추가로, 명령 라인은 종종 멀리 떨어진 컴퓨터 혹은 슈퍼컴퓨터와 상호작용하는 가장 쉬운 방법이다. 고성능 컴퓨팅 시스템에 포함된 다양한 특화된 도구와 자원을 실행하는데 쉘과 친숙성이 거의 필연적이다. 클러스트 컴퓨팅과 클라우드 컴퓨팅이 과학 데이터 클런칭(scientific data cruching)이 점점 대중화됨에 따라 원격 컴퓨터를 구동하는 것이 필수적인 기술이 되어가고 있다. 여기서 다뤄지는 명령-라인 기술에 기반해서 광범위한 과학적 질문과 컴퓨터적 도전과제를 처리할 수 있다."
  },
  {
    "objectID": "shell-intro.html#shell-nelle",
    "href": "shell-intro.html#shell-nelle",
    "title": "3  쉘(Shell) 소개",
    "section": "3.7 실무 사례: 넬 박사 파이프라인",
    "text": "3.7 실무 사례: 넬 박사 파이프라인\n해양 생물학자 넬 니모(Nell Nemo) 박사가 방금전 6개월간 북태평양 소용돌이꼴 조사를 마치고 방금 귀환했다. 태평양 거대 쓰레기 지대에서 젤리같은 해양생물을 표본주출했다. 총 합쳐서 1,520개 시료가 있고 다음 작업이 필요하다:\n\n서로 다른 300개 단백질의 상대적인 함유량을 측정하는 분석기계로 시료를 시험한다. 한 시료에 대한 컴퓨터 출력결과는 각 단백질에 대해 한 줄 파일형식으로 표현된다.\ngoostat으로 명명된 그녀의 지도교수가 작성한 프로그램을 사용하여 각 단백질에 대한 통계량을 계산한다.\n다른 대학원 학생중 한명이 작성한 goodiff로 명명된 프로그램을 사용해서, 각 단백질에 대한 통계량과 다른 단백질에 대해 상응하는 통계량을 비교한다.\n결과를 작성한다. 그녀의 지도교수는 이달 말까지 이 작업을 정말로 마무리해서, 논문이 다음번 Aquatic Goo Letters 저널 특별판에 게재되기를 희망한다.\n\n각 시료를 분석장비가 처리하는데 약 반시간 정도 소요된다. 좋은 소식은 각 시료를 준비하는데는 단지 2분만 소요된다. 연구실에 병렬로 사용할 수 있는 분석장비 8대가 있어서, 이 단계는 약 2주정도만 소요될 것이다.\n나쁜 소식은 goostat, goodiff를 수작업으로 실행한다면, 파일이름 입력하고 “OK” 버튼을 45,150번 눌려야 된다는 사실이다 (goostat 300회 더하기 goodiff \\(\\frac{300 \\times 299}{2}\\)). 매번 30초씩 가정하면 2주 이상 소요될 것이다. 논문 마감일을 놓칠 수도 있지만, 이 모든 명령어를 올바르게 입력할 가능성은 거의 0 에 가깝다.\n다음 수업 몇개는 대신에 그녀가 무엇을 해야되는지 탐색한다. 좀더 구체적으로, 처리하는 파이프라인 중간에 반복되는 작업을 자동화하는데 쉘 명령어(command shell)를 어떻게 사용하는지 설명해서, 논문을 쓰는 동안에 컴퓨터가 하루에 24시간 작업한다. 덤으로 중간 처리작업 파이프라인을 완성하면, 더 많은 데이터를 얻을 때마다 다시 재사용할 수 있게 된다."
  },
  {
    "objectID": "shell-intro.html#쉘-gpt",
    "href": "shell-intro.html#쉘-gpt",
    "title": "3  쉘(Shell) 소개",
    "section": "3.8 쉘 GPT",
    "text": "3.8 쉘 GPT\nOpenAI의 ChatGPT(GPT-3.5)는 콘텐츠 생성에 주된 방점이 있지만 다양한 프로그래밍 코드도 작성함은 물론 유닉스 쉘 프로그램도 작성하여 CLI 생산성을 높이는데 사용될 수 있다. ChatGPT 기능을 활용하여 쉘 명령, 코드 스니펫, 주석, 문서 등을 생성할 수 있다. 즉, 데이터 과학자를 비롯한 개발자가 기존에 업무를 수행하던 방식이 전혀 다르게 된다. 즉, 책, 매뉴얼, 비밀노트(Cheat Sheet), 인터넷 북마크, 구글링 같은 검색없이 바로 터미널에서 바로 정확한 답변을 얻어 귀중한 시간과 노력을 절약할 수 있다.\n예를 들어 앞서 프로젝트를 할 때 해당 유닉스 쉘 명령어가 기억나지 않는다고 하면 shell gpt를 사용하여 해당 작업을 신속하게 수행할 수 있다. shell_gpt는 파이썬 패키지라 다음 파이썬 명령어로 패키지 설치를 할 수 있다.\npip install shell-gpt\n사용방법은 Git Bash를 설치한 후 터미널을 열구 sgpt --shell 다음에 자연어를 넣게 되면 해당되는 쉘 명령어를 알려준다. 이를 실행하게 되면 유닉스 쉘을 이용하여 해당 자동화 작업에 생산성을 높일 수 있다. 현재는 한국어는 지원하지 않아 한글로 작성한 다음 번역기를 사용하여 영어로 입력해야 하고 결과를 얻게 되면 이를 실행하는 방식으로 활용한다.\n$ sgpt --shell 'List the contents of the current directory and display a special character at the end of each filename to indicate its file type.'\nls -F\n\n$ ls -F\n_quarto.yml    cover.png        index.qmd       shell-create.Rmd   shell-loop.Rmd\n00_setup.qmd   docs/            LICENSE         shell-filedir.Rmd  shell-pipefilter.Rmd\n01_openAI.qmd  gpt-shell.Rproj  references.bib  shell-find.Rmd     shell-script.Rmd\ncode/          images/          references.qmd  shell-intro.qmd"
  }
]