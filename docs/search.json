[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gpt-shell",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "00_setup.html",
    "href": "00_setup.html",
    "title": "1  환경설정",
    "section": "",
    "text": "2 OpenAI 모형\nOpenAI에서 제공하는 다양한 모델을 확인할 수 있다. system이 소유한 GPT 모형을 살펴보자.\n프로그램 코드(code) 관련된 GPT 모형도 확인할 수 있다."
  },
  {
    "objectID": "00_setup.html#가상환경-설정",
    "href": "00_setup.html#가상환경-설정",
    "title": "1  환경설정",
    "section": "1.1 가상환경 설정",
    "text": "1.1 가상환경 설정\n다양한 가상환경이 있어 필요한 패키지를 사용하여 파이썬 가상환경을 구축한다. 파이썬 3.3 버전부터 내장된 venv, 많이 사용되는 virtualenvwrapper, virtualenv 등이 유명하다. 본인 취향에 맞는 가상환경을 특정하여 업무에 사용한다. 다음은 venv를 사용해서 가상 개발환경을 구축하는 것을 예시로 보여주고 있다.\n## 디렉토리 생성 및 프로젝트 디렉토리 이동\nmkdir myproject\ncd myproject\n\n## 가상환경 생성\npython -m venv myenv\n\n## 가상환경 활성화\nmyenv\\Scripts\\activate # 윈도우즈\nsource myenv/bin/activate # 리눅스/맥\n\n## 가상환경 비활성화\ndeactivate"
  },
  {
    "objectID": "00_setup.html#api-key-얻기",
    "href": "00_setup.html#api-key-얻기",
    "title": "1  환경설정",
    "section": "1.2 API KEY 얻기",
    "text": "1.2 API KEY 얻기\n가상환경을 구축한 다음 OpenAI에서 제공하는 공식 API에 접근할 수 있는 API 키를 생성하는 것이다. https://openai.com/api/ 1 로 이동하여 계정을 만듭니다.\n안내에 따라 계정을 생성한 다음 https://platform.openai.com/account/api-keys 2 로 이동하여 API 키를 생성한다.\nAPI 키는 조직에 속해야 하며, 조직을 생성하라는 메시지가 표시되는 경우 조직명을 입력한다. 하나의 조직에 속한 경우 조직 ID(Organization ID)를 별도 생성할 필요는 없다. OpenAI 계정을 통해서는 생성한 API KEY는 다시 볼 수 없기 때문에 생성한 비밀 키를 안전하고 접근하기 쉬운 곳에 저장한다."
  },
  {
    "objectID": "00_setup.html#api-key-저장",
    "href": "00_setup.html#api-key-저장",
    "title": "1  환경설정",
    "section": "1.3 API KEY 저장",
    "text": "1.3 API KEY 저장\nAPI KEY를 환경변수로 지정하여 호출하는 방식도 있고, 작업 프로젝트 디렉토리에 로컬 파일에 저장하여 사용하는 방식도 있다. 먼저 윈도우에서 시스템으로 들어가서 환경 변수로 지정하면 해당 변수(OPENAI_API_KEY)를 다양한 프로그램에서 호출하여 사용할 수 있다.\n\n다른 방식은 .env와 같은 파일을 프로젝트 디렉토리 아래 숨긴 파일에 지정하여 사용하는 방식이다. 이런 경우 .gitignore 파일에 버전제어 대상에서 제외시켜 두는 것을 필히 기억한다."
  },
  {
    "objectID": "00_setup.html#헬로월드",
    "href": "00_setup.html#헬로월드",
    "title": "1  환경설정",
    "section": "1.4 헬로월드",
    "text": "1.4 헬로월드\nOpenAI API KEY도 준비가 되었으면 헬로월드 프로그램을 작성해보자. 개발자가 하나의 조직에 속한 경우, API KEY를 운영체제 환경변수로 지정한 경우 다음과 같이 시스템 환경에서 OPENAI_API_KEY 키를 가져와서 OpenAI에서 제공하는 모델목록을 확인할 수 있다.\n\nimport os\nimport openai\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# API 호출 및 모델 목록 출력\nmodels = openai.Model.list()\nprint(models['data'][0])\n\n{\n  \"created\": 1649358449,\n  \"id\": \"babbage\",\n  \"object\": \"model\",\n  \"owned_by\": \"openai\",\n  \"parent\": null,\n  \"permission\": [\n    {\n      \"allow_create_engine\": false,\n      \"allow_fine_tuning\": false,\n      \"allow_logprobs\": true,\n      \"allow_sampling\": true,\n      \"allow_search_indices\": false,\n      \"allow_view\": true,\n      \"created\": 1669085501,\n      \"group\": null,\n      \"id\": \"modelperm-49FUp5v084tBB49tC4z8LPH5\",\n      \"is_blocking\": false,\n      \"object\": \"model_permission\",\n      \"organization\": \"*\"\n    }\n  ],\n  \"root\": \"babbage\"\n}\n\n\n다른 방식은 로컬 파일에 API KEY와 ORG ID 를 저장하고 이를 불러와서 개발에 사용하는 방식이다.\n\nimport os\nimport openai\n\n# .env 파일에서 API_KEY 와 ORG_ID 을 읽어온다.\nwith open(\".env\") as lines:\n  for line in lines:\n    key, value = line.strip().split(\"=\")\n    os.environ[key] = value\n    \n# api_key와 organization 지정\nopenai.api_key = os.environ.get(\"API_KEY\")\nopenai.organization = os.environ.get(\"ORG_ID\")\n\n# API 호출 및 모델 목록 출력\ngpt_models = openai.Model.list()\n\nprint(gpt_models['data'][0])\n\n{\n  \"created\": 1649358449,\n  \"id\": \"babbage\",\n  \"object\": \"model\",\n  \"owned_by\": \"openai\",\n  \"parent\": null,\n  \"permission\": [\n    {\n      \"allow_create_engine\": false,\n      \"allow_fine_tuning\": false,\n      \"allow_logprobs\": true,\n      \"allow_sampling\": true,\n      \"allow_search_indices\": false,\n      \"allow_view\": true,\n      \"created\": 1669085501,\n      \"group\": null,\n      \"id\": \"modelperm-49FUp5v084tBB49tC4z8LPH5\",\n      \"is_blocking\": false,\n      \"object\": \"model_permission\",\n      \"organization\": \"*\"\n    }\n  ],\n  \"root\": \"babbage\"\n}"
  },
  {
    "objectID": "01_openAI.html#텍스트-완성",
    "href": "01_openAI.html#텍스트-완성",
    "title": "2  GPT",
    "section": "2.1 텍스트 완성",
    "text": "2.1 텍스트 완성\nGPT를 사용하여 다양한 작업을 수행할 수 있지만 가장 기본적인 작업은 글쓰기다. GPT가 생성형 AI로 해당 텍스트를 주어지면 나머지 텍스트를 해당 최대 토큰 크기(max_tokens) 길이만큼 텍스트를 생성해준다.\n\nimport os\nimport openai\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ncomplete_next = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"나의 살던 고향은\",\n  max_tokens=7,\n  temperature=0)\n  \ncomplete_next['choices'][0]['text']\n\n'\\n\\n나의'\n\n\n토큰 크기를 100으로 지정하면 제법 긴 텍스트를 출력한다. 영어 토큰에 최적화되어 있는 관계로 한글의 경우 토큰 낭비(?)가 심한 것으로 보인다. 고로 비용이 제법 나가는 점은 한국어로 작업을 할 때 고려해야만 된다.\n\ncomplete_next_100 = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"나의 살던 고향은\",\n  max_tokens=100,\n  temperature=0)\n  \ncomplete_next_100['choices'][0]['text']\n\n'\\n\\n나의 고향은 전라도 익산입니다. 익산은 전라도의 중부에 위치한 도시로, 전라도의 중'"
  },
  {
    "objectID": "01_openAI.html#키워드-추출",
    "href": "01_openAI.html#키워드-추출",
    "title": "2  GPT",
    "section": "2.2 키워드 추출",
    "text": "2.2 키워드 추출\n조금더 흥미로운 주제로 해당 문서를 제시하고 관련 텍스트의 주요 키워드를 추출해보자. Attention Is All You Need 논문(Vaswani et al. 2017)은 AI 분야에서 획기적인 논문으로 평가받있지만 별도 키워드는 제시되고 있지 않아 논문 초록을 앞에 제시하고 Keywords:를 뒤에 두고 논문의 주요 키워드를 추출하게 한다.\n\nprompt_keywords = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n keywords:\"\n\nkeywords = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=prompt_keywords,\n  temperature = 0.5,\n  max_tokens  = 50)\n\nkeywords['choices'][0]['text']\n\nmax_tokens을 50으로 제한하여 temperature = 0.5로 너무 창의적이지 않게 키워드 추출 작업을 지시한 경우 다음과 같은 결과를 어덱 된다.\n'\\nSequence transduction, neural networks, attention mechanisms, machine translation, parsing'\n이번에는 한글 논문초록에서 키워드를 추출해보자. 2020년 출간된 논문(lee2020?)의 한글 초록에서 제시된 키워드와 OpenAI GPT가 제시하고 있는 키워드와 비교해보자.\n\n논문 소스코드: 바로가기\nPDF 출판 논문: 다운로드\n\n\nprompt_keywords = \"알파고가 2016년 바둑 인간 챔피언 이세돌 9단을 현격한 기량차이로 격파하면서 인공지능에 대한 관심이 급격히 증가하였다. 그와 동시에 기계가 인간의 일자리 잠식을 가속화하면서 막연한 불안감이 삽시간에 전파되었다. 기계와의 일자리 경쟁은 컴퓨터의 출현이전부터 시작되었지만 인간만의 고유한 영역으로 알고 있던 인지, 창작 등 다양한 분야에서 오히려 인간보다 더 우수한 성능과 저렴한 가격 경쟁력을 보여주면서 기존 인간의 일자리가 기계에 대체되는 것이 가시권에 들었다. 이번 문헌조사와 실증 데이터 분석을 통해서 기계가 인간의 일자리를 대체하는 자동화의 본질에 대해서 살펴보고, 인간과 기계의 업무 분장을 통해 더 생산성을 높일 수 있는 방안을 제시하고자 한다.\\n\\n 키워드:\"\n\nkeywords = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=prompt_keywords,\n  temperature = 0.5,\n  max_tokens  = 100)\n\nkeywords['choices'][0]['text']\n\n' 인공지능, 자동화, 인간과 기계의 업무 분장, 생산성 \\n\\n이 문헌조사는 인공지능이 인'\nGPT가 생성한 키워드를 논문저자가 추출한 키워드와 비교하면 다소 차이가 있지만 그래도 상위 3개 키워드는 높은 일치도를 보이고 있다.\n\n\n\nOpenAI GPT 키워드\n\n인공지능\n자동화\n인간과 기계의 업무 분장\n생산성 문헌조사는 인공지능이 인’\n\n\n\n\n논문저자 추출\n\n자동화\n데이터 과학\n인공지능\n일자리\n기계와 사람의 업무분장\n\n\n\n\nGPT-4는 더 높은 성능을 보여주고 있다. https://chat.openai.com/chat?model=gpt-4에 해당 텍스트를 던져주면 다음과 같이 키워드를 추출하고 요약을 해준다."
  },
  {
    "objectID": "01_openAI.html#텍스트-요약",
    "href": "01_openAI.html#텍스트-요약",
    "title": "2  GPT",
    "section": "2.3 텍스트 요약",
    "text": "2.3 텍스트 요약\nAttention Is All You Need 논문(Vaswani et al. 2017) 초록은 https://platform.openai.com/tokenizer 계산기를 통해 230개 토큰 1,138 문자로 작성된 것이 확인된다. 영어 기준 다음과 같은 맥락을 이해하고 이를 대략 20% 수준 50 토큰으로 줄여보자.\n\n100 토큰은 대략 75 단어\n평균 단어는 대략 5 문자로 구성\n100 토큰은 375개 문자\n\n\nprompt_keywords = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n summary:\"\n\nkeywords = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=prompt_keywords,\n  temperature = 0.5,\n  max_tokens  = 50)\n\nkeywords['choices'][0]['text']\n\n상기 논문 초록을 50개 토큰으로 요약하면 다음과 같이 48개 토큰, 277 문자수로 요약해준다.\n'\\n\\nThe Transformer is a new neural network architecture based solely on attention mechanisms, which is shown to outperform existing models on two machine translation tasks. It is more parallelizable and requires significantly less time to train than existing models, achieving a B'"
  },
  {
    "objectID": "01_openAI.html#여론조사-할일-생성",
    "href": "01_openAI.html#여론조사-할일-생성",
    "title": "2  GPT",
    "section": "2.4 여론조사 할일 생성",
    "text": "2.4 여론조사 할일 생성\n\ntodo_list = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"여론조사를 위해서 해야될 일을 작성하세요\\n\\n1.\",\n  temperature=0.3,\n  max_tokens = 1000,\n  top_p = 0.1,\n  frequency_penalty=0,\n  presence_penalty=0.5,\n  stop=[\"6.\"]\n)\n\ntodo_list['choices'][0]['text']\n\n' 여론조사 대상자를 선정하고, 여론조사 대상자의 수를 결정합니다.\\n\\n2. 여론조사 대상자들에게 여론조사 질문지를 배포합니다.\\n\\n3. 여론조사 대상자들에게 여론조사 응답을 요청합니다.\\n\\n4. 여론조사 응답을 수집하고, 분석합니다.\\n\\n5. 여론조사 결과를 보고서로 작성합니다.'\n\n\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” https://arxiv.org/abs/1706.03762."
  }
]