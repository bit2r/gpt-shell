

\begin{shadequote}[r]{존 튜키(John Tukey)}
데이터 분석가는 단순한 그래프를 통해 다른 어떤 기기보다 많은 정보를 얻을 수 있다. (The simple graph has brought more information to the data analyst’s mind than any other device.)
\end{shadequote}


# 챗GPT 데이터 과학 {#cli-ds-gpt}

1950년대부터 본격적으로 컴퓨터가 도입되면서 CLI를 필두로 
다양한 사용자 인터페이스(User Interface)가 적용되었다.
스티브 잡스의 애플사는 매킨토시 GUI에 이어 아이폰 모바일 인터페이스를 
일반화시켰다면, 최근 챗GPT는 언어 사용자 인터페이스(LUI)를 통해
각 분야에 혁신을 예고하고 있다. 챗GPT 데이터 과학도 사용자 관점에서 본다면
기존 R, 파이썬, SQL, 엑셀 등 데이터 과학 구문을 머리속에 암기하고 있거나
구글이나 네이버를 통해 중요 키워드를 통해 문제를 해결해야 했지만, 이제 챗GPT가
자연어를 이해하기 때문에 데이터 전처리, 통계 작업, 데이터 분석, 시각화, 모형개발 등 
데이터 과학 전반에 변화는 필연적이다.\index{User Inferface!CLI}\index{User Inferface!GUI}\index{User Inferface!WUI}\index{User Inferface!MUI}
\index{User Inferface!LUI}\index{사용자 인터페이스}

```{mermaid}
flowchart TB
    
    subgraph A["사용자 인터페이스"]
    direction LR
        CLI["CLI 1950~<br>Command Line Interface<br> GUI 1970~<br> Graphic User Inferface"]     --> WIMI
        WIMI["Web Interface <br> 1994~ <br> Mobile Interface <br> 2007~"] --> NUI["LUI 2023~<br> Language <br> User Interface"]
    end

classDef default fill:#f9f7f7,stroke:#333,stroke-width:2px;
classDef A font-size:16px;

```

## 실습 데이터

`palmerpenguins` 패키지에 있는 `penguins` 데이터셋이 붓꽃(`iris`) 데이터셋을 대체하며 
데이터 과학에서 많이 사용되고 있다. 챗GPT 데이터 과학 접근법을 보여주기 위해 먼저 
`palmerpenguins` 패키지에 `penguins` 데이터셋을 펭귄 종별로 명칭을 달리하여 
`.csv` 파일로 저장한다. 
챗GPT 초창기 한국어를 영어로 번역해야 성능이 좋아졌으나 
최근에는 한국어로 프롬프트를 작성해도 좋은 결과를 제시하고 있다.

:::{.callout-note}
### 파일 저장 프롬프트

"palmerpenguins 패키지 penguins 데이터셋을 각 종별로 나눠 \
.csv 파일로 저장하는 R 코드를 작성해 주세요. purrr 패키지 walk 함수를  사용해서 \
작성해주세요."
:::

GPT-4 모형엔진을 사용하면 다음에 저자가 직접 작성한 코드와 유사한 R 코드를 작성해준다.
`data/` 디렉토리 아래 `penguin_adelie.csv`, `penguin_chinstrap.csv`, `data/penguin_gentoo.csv` 
파일 세개로 나눠진 것을 `fs` 패키지 `dir_ls()` 함수를 통해 확인된다.

```{r}
# *. 패키지 --------------------------------------------------------------

library(palmerpenguins)
library(tidyverse)

# *. 데이터 나누기 --------------------------------------------------------------

species <- palmerpenguins::penguins %>%
  count(species) %>%
  pull(species) %>%
  as.character(.)

save_species <- function(species_name = "Adelie") {
  palmerpenguins::penguins %>%
    filter(str_detect(species, species_name)) %>%
    write_csv(glue::glue("data/penguin_{str_to_lower(species_name)}.csv"))
}

walk(species, save_species)

fs::dir_ls(path = "data/", glob="data/penguin_*.csv")
```

## 펭귄종별 개체수 

펭귄 종별로 `.csv` 파일을 나눈 이유는 실제 현장에서 다양하게 쪼개진 데이터를 
하나로 묶어 분석을 수행하는 경우가 많은 일일이 데이터를 열어 분석가가 직접
분석하는 것은 손이 많이 가고 경우에 따라서는 실수가 들어가기 때문에 이를 방지할 필요가 있다.\index{sgpt}

펭귄 종별 개체수 빈도수를 계산하는 유닉스 쉘 프로그램을 작성해보자.

:::{.callout-note}
### 첫번째 분석 프롬프트

"펭귄 종별로 구분된 palmer penguins csv 파일이 세개 data/ 폴더안에 있습니다. \
각각의 파일명은 penguin_adelie.csv, penguin_chinstrap.csv, penguin_gentoo.csv 으로 \
종별 펭귄 개체수 빈도수를 계산하는 코드를 작성하세요."
:::

첫번째 분석 프롬프트를 작성하고 ShellGPT에 실행하면 다음과 같은 결과가 나온다.
`sgpt`에서 제시한 유닉스 쉘 명령어를 해석하면 "이 셸 명령은 세 개의 서로 다른 CSV 파일에서 펭귄 데이터를 읽고, 종 열을 추출하고, 고유 값을 정렬 및 계산한 후 결과를 출력합니다."라는 취지로 작성된 것을 확인하고 이를 `[E]xecute`로 실행하면 기대했던 결과를 얻을 수 없음이 확인된다.

```bash
$ sgpt -s "펭귄 종별로 구분된 palmer penguins csv 파일이 세개 data/ 폴더안에 있습니다. \
> 각각의 파일명은 penguin_adelie.csv, penguin_chinstrap.csv, penguin_gentoo.csv 으로 \
> 종별 펭귄 개체수 빈도수를 계산하는 코드를 작성하세요."
cat data/penguin_adelie.csv | tail -n +2 | cut -d ',' -f 2 | sort | uniq -c
cat data/penguin_chinstrap.csv | tail -n +2 | cut -d ',' -f 2 | sort | uniq -c
cat data/penguin_gentoo.csv | tail -n +2 | cut -d ',' -f 2 | sort | uniq -c
[E]xecute, [D]escribe, [A]bort: D
This shell command reads penguin data from three different CSV files, extracts the species column, sorts and counts the unique values, and outputs the results.
[E]xecute, [D]escribe, [A]bort: E
     44 Biscoe
     56 Dream
     52 Torgersen
     68 Dream
    124 Biscoe
```

두번째 분석 프롬프트에서는 명확하게 펭귄종(`species`)이 위치한 칼럼을 특정하고 쉘 명령어를 작성하도록 지시하고 이를 바로 실행해보자.

:::{.callout-note}
### 두번째 분석 프롬프트

"펭귄 종별로 구분된 palmer penguins csv 파일이 세개 data/ 폴더안에 있습니다. \
각각의 파일명은 penguin_adelie.csv, penguin_chinstrap.csv, penguin_gentoo.csv 으로 \
종은 species 첫번째 칼럼에 위치하고, 종별 펭귄 개체수 빈도수를 계산하는 코드를 작성하세요."
:::

원하는 결과가 나오기는 하지만 `species`도 각 1 빈도수로 출력되는데 이유는 `.csv` 파일에 칼럼명도 펭귄 개체로 인식해서 함께 빈도수를 계산했기 때문이다. 이에 대한 문제도 제기하고 프롬프트를 수정해보자.

```bash
$ sgpt -s "펭귄 종별로 구분된 palmer penguins csv 파일이 세개 data/ 폴더안에 있습니다. \
> 각각의 파일명은 penguin_adelie.csv, penguin_chinstrap.csv, penguin_gentoo.csv 으로 \
> 종은 species 첫번째 칼럼에 위치하고, 종별 펭귄 개체수 빈도수를 계산하는 코드를 작성하세요."
cat data/penguin_adelie.csv | cut -d ',' -f 1 | sort | uniq -c
cat data/penguin_chinstrap.csv | cut -d ',' -f 1 | sort | uniq -c
cat data/penguin_gentoo.csv | cut -d ',' -f 1 | sort | uniq -c
[E]xecute, [D]escribe, [A]bort: E
    152 Adelie
      1 species
     68 Chinstrap
      1 species
    124 Gentoo
      1 species
```

첫번째 행이 칼럼명임을 명시해서 세번째 프롬프트를 작성한다. 하지만 제시된 프롬프트만으로 깔끔한 결과를 제시하지 못하고 있다.

:::{.callout-note}
### 세번째 분석 프롬프트
"펭귄 종별로 구분된 palmer penguins csv 파일이 세개 data/ 폴더안에 있습니다. \
각각의 파일명은 penguin_adelie.csv, penguin_chinstrap.csv, penguin_gentoo.csv 으로 \
종은 species 첫번째 칼럼에 위치하고, 첫번째 행은 칼럼명이고, 종별 펭귄 개체수 빈도수를 계산하는 코드를 작성하세요."
:::

```bash
$ sgpt -s "펭귄 종별로 구분된 palmer penguins csv 파일이 세개 data/ 폴더안에 있습니다. \
> 각각의 파일명은 penguin_adelie.csv, penguin_chinstrap.csv, penguin_gentoo.csv 으로 \
> 종은 species 첫번째 칼럼에 위치하고, 첫번째 행은 칼럼명이고, 종별 펭귄 개체수 빈도수를 계산하는 코드를 작성하세요."
awk -F"," 'NR>1{a[$1]++} END{for(i in a) print i,a[i]}' data/penguin_adelie.csv data/penguin_chinstrap.csv data/penguin_gentoo.csv
[E]xecute, [D]escribe, [A]bort: D
Count the number of occurrences of the first column in three CSV files and print the results.
[E]xecute, [D]escribe, [A]bort: E
Gentoo 124
Chinstrap 68
Adelie 152
species 2
```

맥락을 유지하여 원하는 결과를 얻을 수 있는 챗GPT 채팅 인터페이스를 사용하여 결과를 도출할 수 있도록 작업해보자.
먼저 `sgpt`가 제시한 유닉스 쉘 스크립트를 챗GPT를 통해 의미를 파악해보자.

:::{.callout-note}
### 코드해석 프롬프트 

> 다음 유닉스 쉘 코드를 해석해주세요.
> awk -F"," 'NR>1{a[$1]++} END{for(i in a) print i,a[i]}' data/penguin_adelie.csv data/penguin_chinstrap.csv data/penguin_gentoo.csv

\rule{7cm}{0.3mm}

주어진 awk 명령은 3개의 CSV 파일(penguin_adelie.csv, penguin_chinstrap.csv, penguin_gentoo.csv)에 대해 동작하는데, 이 파일들의 각 줄을 처리합니다. -F","는 각 라인이 쉼표(,)로 구분된 필드로 구성된 CSV 형식임을 의미합니다.

- awk 스크립트의 각 부분을 분석하면 다음과 같습니다:

- NR>1 {a[$1]++}: NR은 현재까지 읽은 라인 수를 나타내는 변수입니다. NR>1이라는 조건은 첫 번째 행(즉, 헤더 행)을 건너뛰도록 합니다. 그 다음에 오는 {a[$1]++}는 첫 번째 필드($1)의 값을 key로 사용하여 배열 a의 해당 요소의 값을 1 증가시킵니다. 다시 말해, 이 명령은 각 파일에서 첫 번째 필드의 값이 나타나는 횟수를 계산합니다.

- END {for(i in a) print i,a[i]}: 이 부분은 모든 입력을 처리한 후 실행되는 명령입니다. END 부분에서는 for 반복문을 사용하여 배열 a의 모든 요소를 반복하고, 각 key(i)와 그에 대응하는 값(a[i])을 출력합니다.

결국, 이 awk 명령은 3개의 CSV 파일에서 첫 번째 필드의 각 값이 나타나는 횟수를 출력합니다. 첫 번째 행(헤더)은 건너뜁니다. 이 코드는 첫 필드가 범주형 변수(예: 특정 분류)인 경우 해당 변수의 각 값의 출현 빈도를 확인하는 데 유용할 수 있습니다.

:::

`species 2`를 제거하는 유닉스 쉘 코드를 작성하는 것이 목표라서 이를 달성할 수 있도록 프롬프트에 직설적으로 작성한다.

:::{.callout-note}
### `species 2` 제거 프롬프트

> 쉘 코드를 실행하면 다음과 같은 결과가 나옵니다.  species 2 를 제외시키도록 코드를 다시 작성해주세요.
> 
> Gentoo 124
> Chinstrap 68
> Adelie 152
> species 2

\rule{7cm}{0.3mm}

주어진 출력에서 'species'라는 키 값이 있습니다. 이 값은 아마도 csv 파일의 헤더에서 오는 값일 것입니다. 그렇다면, awk 명령을 수정하여 'species'를 무시하도록 할 수 있습니다. 다음은 수정된 코드입니다:

```bash
awk -F"," 'NR>1 && $1 != "species" {a[$1]++} END{for(i in a) print i,a[i]}' data/penguin_adelie.csv data/penguin_chinstrap.csv data/penguin_gentoo.csv
```

이 코드는 원래의 코드에서 추가된 && $1 != "species" 조건을 통해, 첫 번째 필드의 값이 "species"인 행은 건너뛰게 됩니다. 이렇게 하면 'species'라는 키 값이 결과에서 제외되게 됩니다.

:::

챗GPT를 통해 얻은 명령어를 복사하여 쉘 프롬프트에 붙여넣고 실행시키면 원하는 결과를 얻은 것이 확인된다.

```bash
$ awk -F"," 'NR>1 && $1 != "species" {a[$1]++} END{for(i in a) print i,a[i]}' data/penguin_adelie.csv data/penguin_chinstrap.csv data/penguin_gentoo.csv
Gentoo 124
Chinstrap 68
Adelie 152
```

R 코드를 통해 세개로 쪼개진 `.csv` 파일이 목표를 달성했는지 검산해보자. 
펭귄 종 순서는 다르지만 정확하게 펭귄종별 관측 빈도수가 일치함을 확인할 수 있다.

```{r}
palmerpenguins::penguins %>%
  count(species)
```

